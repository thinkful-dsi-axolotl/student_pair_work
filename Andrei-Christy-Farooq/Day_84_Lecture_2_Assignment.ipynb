{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Day 84 Lecture 2 Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQfS-31Jh-bL",
        "colab_type": "text"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "In this assignment, we will learn about recurrent neural networks. We will create an RNN and learn to classify text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyi-pLDUxxLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jNy0eS3rWq8",
        "colab_type": "code",
        "outputId": "f0313142-16a6-4b8b-af7d-bc17b4bceeb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Install tf2.0\n",
        "!pip install --upgrade tensorflow --quiet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 421.8MB 35kB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 38.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOy993_Xh-bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToNRsZf5h-bP",
        "colab_type": "code",
        "outputId": "7772ccf9-de71-49d8-d191-879c8b0844ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "yelp = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/yelp_labeled.csv', error_bad_lines=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 281: expected 2 fields, saw 3\\nSkipping line 290: expected 2 fields, saw 3\\nSkipping line 296: expected 2 fields, saw 3\\nSkipping line 322: expected 2 fields, saw 3\\nSkipping line 373: expected 2 fields, saw 3\\nSkipping line 417: expected 2 fields, saw 3\\nSkipping line 427: expected 2 fields, saw 3\\nSkipping line 429: expected 2 fields, saw 3\\nSkipping line 577: expected 2 fields, saw 3\\nSkipping line 578: expected 2 fields, saw 3\\nSkipping line 611: expected 2 fields, saw 3\\nSkipping line 677: expected 2 fields, saw 3\\nSkipping line 771: expected 2 fields, saw 3\\nSkipping line 930: expected 2 fields, saw 3\\nSkipping line 979: expected 2 fields, saw 4\\nSkipping line 980: expected 2 fields, saw 3\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fKQfObOh-bX",
        "colab_type": "code",
        "outputId": "711bb6f3-f5be-4baf-ff6e-110f18966ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "yelp.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0                           Wow... Loved this place.          1\n",
              "1                                 Crust is not good.          0\n",
              "2          Not tasty and the texture was just nasty.          0\n",
              "3  Stopped by during the late May bank holiday of...          1\n",
              "4  The selection on the menu was great and so wer...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXyJFyo9h-bZ",
        "colab_type": "code",
        "outputId": "95990654-f63f-4cf3-c662-95e29bf1cf4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "yelp.sentiment.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    494\n",
              "0    482\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp0a4qU9h-bb",
        "colab_type": "text"
      },
      "source": [
        "We have loaded a Yelp review dataset above. A positive sentiment is classified as 1 and a negative sentiment is classified as 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyboex_Vh-bb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def remove_stopwords(input_text):\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words)       \n",
        "\n",
        "def stem_list(word_list):\n",
        "    stemmed = []\n",
        "    for word in word_list:\n",
        "        stemmedword = stemmer.stem(word)\n",
        "        stemmed.append(stemmedword)\n",
        "    return stemmed\n",
        "\n",
        "def normalize(terms):\n",
        "    terms = terms.lower()\n",
        "    terms = remove_stopwords(terms)\n",
        "    word_delimiters = u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013 ]'\n",
        "    term_list = re.split(word_delimiters, terms)\n",
        "    trimmed = [x.rstrip() for x in term_list]\n",
        "    stemmed = stem_list(trimmed)\n",
        "    space = ' '\n",
        "    normed = space.join(stemmed)\n",
        "    normed = normed.replace('  ', ' ')\n",
        "    return normed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8HG4hUM-Vja",
        "colab_type": "code",
        "outputId": "0e55cc0c-fb51-438f-ca26-6d98b7b62f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsuGlOW8h-bd",
        "colab_type": "text"
      },
      "source": [
        "In the code block above, we have functions to remove stopwords, stem, and normalize the text (remove special characters and trim white space). Apply the normalize function to every yelp review and assign the normalized text to a new column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDWbek-Yh-be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Answer below:\n",
        "yelp['proc_text'] = yelp['text'].apply(normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dwLKZDvtnsU",
        "colab_type": "code",
        "outputId": "23aca9bc-0d39-4a12-a152-0d23b6bfd980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "yelp.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>proc_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "      <td>wow  love place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "      <td>crust not good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "      <td>not tasti textur nasti</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "      <td>stop late may bank holiday rick steve recommen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>select menu great price</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                          proc_text\n",
              "0                           Wow... Loved this place.  ...                                   wow  love place \n",
              "1                                 Crust is not good.  ...                                    crust not good \n",
              "2          Not tasty and the texture was just nasty.  ...                            not tasti textur nasti \n",
              "3  Stopped by during the late May bank holiday of...  ...  stop late may bank holiday rick steve recommen...\n",
              "4  The selection on the menu was great and so wer...  ...                           select menu great price \n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tywlufjyh-bf",
        "colab_type": "text"
      },
      "source": [
        "Next, use the one hot function for text encoding and encode the normalized text. Determine the vocabulary size to perform the encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWKnOSd_h-bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Answer below:\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjg4C7LCzHq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(set(yelp.proc_text.str.split().sum()))\n",
        "encoded = [one_hot(l, vocab_size) for l in list(yelp['proc_text'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnRMy2S0h-bh",
        "colab_type": "text"
      },
      "source": [
        "Convert the encoded sequences into a numpy array and make sure all reviews are the same length using the `pad_sequences` function in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk24Behch-bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Answer below:\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRpRkVOR0yUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pad_sequences(encoded )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIclszsJh-bj",
        "colab_type": "text"
      },
      "source": [
        "Split the data into train and test. Use 20% for test. The sentiment column should be used as the target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlaZSo-Yh-bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Answer below:\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = data\n",
        "y = yelp.sentiment.values\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2, random_state=84)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAsqg_REh-bm",
        "colab_type": "text"
      },
      "source": [
        "Create a sequential model. The model should contain an embedding layer with input dim that is the size of the largest encoding in the vocabulary. The output dim should be 100, the input length is the number of columns in the training data. \n",
        "After the embedding layer, add a SimpleRNN layer with unit size 32, a dense layer of size 8 and a dense output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBDcbI7_mLdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data,step):\n",
        "    X,y = [], []\n",
        "    for i in range(len(data)-step):\n",
        "        X.append(data[i:d],1)\n",
        "        y.append(data[d,])\n",
        "    return np.array(X), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M8sQ_fMhmGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input_dim = max of len(yelp.text)\n",
        "output_dim = 100\n",
        "inpu_length = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK-rQxyynoV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "step = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssougKQUh-bm",
        "colab_type": "code",
        "outputId": "e2ae0358-f7e5-497c-8aae-83feb9ac1f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Answer below:\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=np.max(X_train)+1,output_dim=X_train.shape[1]))\n",
        "model.add(SimpleRNN(units=32, input_shape=(1,step),activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, None, 83)          135124    \n",
            "_________________________________________________________________\n",
            "simple_rnn_4 (SimpleRNN)     (None, 32)                3712      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 8)                 264       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 139,109\n",
            "Trainable params: 139,109\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjgqWnioh-bo",
        "colab_type": "text"
      },
      "source": [
        "Compile using the optimizer of your choice, use MSE for your loss function. Fit the model using a batch size of 128 and 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWoSKzxA26pt",
        "colab_type": "code",
        "outputId": "cd2e75fa-8808-40ef-9973-4404e26b3a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.max(X_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1627"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewNbf3fKh-bo",
        "colab_type": "code",
        "outputId": "48f97a5e-a156-4134-af18-cdabe48a80a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Answer below:\n",
        "model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
        "# Fit the model: setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=50, verbose=2, validation_data=(X_test, y_test))\n",
        "# Score the model\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 780 samples, validate on 196 samples\n",
            "Epoch 1/50\n",
            "780/780 - 1s - loss: 0.4792 - accuracy: 0.9244 - val_loss: 1.0693 - val_accuracy: 0.7296\n",
            "Epoch 2/50\n",
            "780/780 - 0s - loss: 0.0084 - accuracy: 0.9974 - val_loss: 1.0901 - val_accuracy: 0.7296\n",
            "Epoch 3/50\n",
            "780/780 - 0s - loss: 0.0079 - accuracy: 0.9974 - val_loss: 1.1219 - val_accuracy: 0.7245\n",
            "Epoch 4/50\n",
            "780/780 - 0s - loss: 0.0075 - accuracy: 0.9974 - val_loss: 1.1445 - val_accuracy: 0.7296\n",
            "Epoch 5/50\n",
            "780/780 - 0s - loss: 0.0075 - accuracy: 0.9974 - val_loss: 1.1731 - val_accuracy: 0.7245\n",
            "Epoch 6/50\n",
            "780/780 - 0s - loss: 0.0068 - accuracy: 0.9974 - val_loss: 1.1903 - val_accuracy: 0.7296\n",
            "Epoch 7/50\n",
            "780/780 - 0s - loss: 0.0068 - accuracy: 0.9974 - val_loss: 1.2117 - val_accuracy: 0.7296\n",
            "Epoch 8/50\n",
            "780/780 - 0s - loss: 0.0067 - accuracy: 0.9974 - val_loss: 1.2688 - val_accuracy: 0.7092\n",
            "Epoch 9/50\n",
            "780/780 - 0s - loss: 0.0066 - accuracy: 0.9974 - val_loss: 1.3013 - val_accuracy: 0.7143\n",
            "Epoch 10/50\n",
            "780/780 - 0s - loss: 0.0067 - accuracy: 0.9974 - val_loss: 1.3424 - val_accuracy: 0.7092\n",
            "Epoch 11/50\n",
            "780/780 - 0s - loss: 0.0065 - accuracy: 0.9962 - val_loss: 1.4033 - val_accuracy: 0.6939\n",
            "Epoch 12/50\n",
            "780/780 - 0s - loss: 0.0068 - accuracy: 0.9974 - val_loss: 1.3584 - val_accuracy: 0.7092\n",
            "Epoch 13/50\n",
            "780/780 - 0s - loss: 0.0071 - accuracy: 0.9962 - val_loss: 1.3086 - val_accuracy: 0.7143\n",
            "Epoch 14/50\n",
            "780/780 - 0s - loss: 0.0080 - accuracy: 0.9974 - val_loss: 1.2749 - val_accuracy: 0.7245\n",
            "Epoch 15/50\n",
            "780/780 - 0s - loss: 0.0067 - accuracy: 0.9974 - val_loss: 1.7056 - val_accuracy: 0.7194\n",
            "Epoch 16/50\n",
            "780/780 - 0s - loss: 0.0303 - accuracy: 0.9923 - val_loss: 1.3368 - val_accuracy: 0.7143\n",
            "Epoch 17/50\n",
            "780/780 - 0s - loss: 0.0060 - accuracy: 0.9974 - val_loss: 1.3864 - val_accuracy: 0.7143\n",
            "Epoch 18/50\n",
            "780/780 - 0s - loss: 0.0067 - accuracy: 0.9974 - val_loss: 1.3484 - val_accuracy: 0.7245\n",
            "Epoch 19/50\n",
            "780/780 - 0s - loss: 0.0062 - accuracy: 0.9974 - val_loss: 1.3626 - val_accuracy: 0.7296\n",
            "Epoch 20/50\n",
            "780/780 - 0s - loss: 0.0061 - accuracy: 0.9974 - val_loss: 1.4001 - val_accuracy: 0.7296\n",
            "Epoch 21/50\n",
            "780/780 - 0s - loss: 0.0057 - accuracy: 0.9974 - val_loss: 1.4337 - val_accuracy: 0.7245\n",
            "Epoch 22/50\n",
            "780/780 - 0s - loss: 0.0063 - accuracy: 0.9974 - val_loss: 1.4502 - val_accuracy: 0.7245\n",
            "Epoch 23/50\n",
            "780/780 - 0s - loss: 0.0065 - accuracy: 0.9974 - val_loss: 1.4637 - val_accuracy: 0.7194\n",
            "Epoch 24/50\n",
            "780/780 - 0s - loss: 0.0066 - accuracy: 0.9974 - val_loss: 1.5211 - val_accuracy: 0.7245\n",
            "Epoch 25/50\n",
            "780/780 - 0s - loss: 0.0459 - accuracy: 0.9872 - val_loss: 1.4278 - val_accuracy: 0.7347\n",
            "Epoch 26/50\n",
            "780/780 - 0s - loss: 0.0058 - accuracy: 0.9974 - val_loss: 1.4454 - val_accuracy: 0.7347\n",
            "Epoch 27/50\n",
            "780/780 - 0s - loss: 0.0065 - accuracy: 0.9974 - val_loss: 1.4566 - val_accuracy: 0.7296\n",
            "Epoch 28/50\n",
            "780/780 - 0s - loss: 0.0057 - accuracy: 0.9974 - val_loss: 1.4694 - val_accuracy: 0.7296\n",
            "Epoch 29/50\n",
            "780/780 - 0s - loss: 0.0060 - accuracy: 0.9974 - val_loss: 1.4820 - val_accuracy: 0.7296\n",
            "Epoch 30/50\n",
            "780/780 - 0s - loss: 0.0061 - accuracy: 0.9974 - val_loss: 1.4888 - val_accuracy: 0.7245\n",
            "Epoch 31/50\n",
            "780/780 - 0s - loss: 0.0060 - accuracy: 0.9974 - val_loss: 1.5064 - val_accuracy: 0.7194\n",
            "Epoch 32/50\n",
            "780/780 - 0s - loss: 0.0058 - accuracy: 0.9974 - val_loss: 1.5214 - val_accuracy: 0.7194\n",
            "Epoch 33/50\n",
            "780/780 - 0s - loss: 0.0060 - accuracy: 0.9974 - val_loss: 1.5358 - val_accuracy: 0.7194\n",
            "Epoch 34/50\n",
            "780/780 - 0s - loss: 0.0056 - accuracy: 0.9974 - val_loss: 1.5570 - val_accuracy: 0.7143\n",
            "Epoch 35/50\n",
            "780/780 - 0s - loss: 0.0066 - accuracy: 0.9949 - val_loss: 1.5598 - val_accuracy: 0.7143\n",
            "Epoch 36/50\n",
            "780/780 - 0s - loss: 0.0061 - accuracy: 0.9974 - val_loss: 1.5851 - val_accuracy: 0.7143\n",
            "Epoch 37/50\n",
            "780/780 - 0s - loss: 0.0069 - accuracy: 0.9962 - val_loss: 1.5544 - val_accuracy: 0.7194\n",
            "Epoch 38/50\n",
            "780/780 - 0s - loss: 0.0063 - accuracy: 0.9962 - val_loss: 1.6365 - val_accuracy: 0.7143\n",
            "Epoch 39/50\n",
            "780/780 - 0s - loss: 0.0082 - accuracy: 0.9974 - val_loss: 1.5900 - val_accuracy: 0.7143\n",
            "Epoch 40/50\n",
            "780/780 - 0s - loss: 0.0056 - accuracy: 0.9974 - val_loss: 1.6501 - val_accuracy: 0.7347\n",
            "Epoch 41/50\n",
            "780/780 - 0s - loss: 0.0092 - accuracy: 0.9936 - val_loss: 1.5485 - val_accuracy: 0.7092\n",
            "Epoch 42/50\n",
            "780/780 - 0s - loss: 0.0061 - accuracy: 0.9974 - val_loss: 1.5675 - val_accuracy: 0.7194\n",
            "Epoch 43/50\n",
            "780/780 - 0s - loss: 0.0061 - accuracy: 0.9974 - val_loss: 1.5651 - val_accuracy: 0.7245\n",
            "Epoch 44/50\n",
            "780/780 - 0s - loss: 0.0058 - accuracy: 0.9974 - val_loss: 1.5204 - val_accuracy: 0.7092\n",
            "Epoch 45/50\n",
            "780/780 - 0s - loss: 0.0080 - accuracy: 0.9962 - val_loss: 1.5611 - val_accuracy: 0.7347\n",
            "Epoch 46/50\n",
            "780/780 - 0s - loss: 0.0062 - accuracy: 0.9974 - val_loss: 1.5815 - val_accuracy: 0.7296\n",
            "Epoch 47/50\n",
            "780/780 - 0s - loss: 0.0066 - accuracy: 0.9974 - val_loss: 1.6675 - val_accuracy: 0.7143\n",
            "Epoch 48/50\n",
            "780/780 - 0s - loss: 0.0065 - accuracy: 0.9949 - val_loss: 1.6726 - val_accuracy: 0.7092\n",
            "Epoch 49/50\n",
            "780/780 - 0s - loss: 0.0064 - accuracy: 0.9974 - val_loss: 1.7001 - val_accuracy: 0.7245\n",
            "Epoch 50/50\n",
            "780/780 - 0s - loss: 0.0059 - accuracy: 0.9974 - val_loss: 1.7630 - val_accuracy: 0.7245\n",
            "Test score: 1.7630139519745598\n",
            "Test accuracy: 0.7244898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk88z8R8h-bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpVO5khh4Sds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = tts(yelp['proc_text'],y,test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT5cgdjk4ik3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vect = TfidfVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pybvwO4v4tbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_vect = vect.fit_transform(X_train)\n",
        "x_test_vect = vect.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJimNUaU40ro",
        "colab_type": "code",
        "outputId": "c16040cb-f2af-43d1-cfca-e3f3886f89b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "rfc = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rfc.fit(x_train_vect, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qQltn085RMg",
        "colab_type": "code",
        "outputId": "612af0e0-e072-437d-ba8c-75fb7772d7f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rfc.score(x_test_vect,y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ad42ovf6rNV",
        "colab_type": "code",
        "outputId": "bb6f2e3a-ed38-4b8b-c021-1c7a27d10c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rfc.score(x_train_vect,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    }
  ]
}