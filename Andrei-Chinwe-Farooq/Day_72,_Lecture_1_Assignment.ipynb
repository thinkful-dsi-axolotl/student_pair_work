{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Day 72, Lecture 1: Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iCUbt7u4JIO",
        "colab_type": "text"
      },
      "source": [
        "# Text Data Cleaning and Preprocessing Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuqMJsi-4JIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an6OMKctr54S",
        "colab_type": "code",
        "outputId": "f277abef-6fa4-47a3-c182-f8a500e117c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfOycaqpsKxw",
        "colab_type": "code",
        "outputId": "7875ee6f-00cc-4780-b50e-a158636112fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install feedparser --quiet\n",
        "import json\n",
        "import requests\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▊                              | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 112kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 122kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 133kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 143kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 153kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 163kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 174kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 184kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 3.1MB/s \n",
            "\u001b[?25h  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCY5p4L54JIY",
        "colab_type": "text"
      },
      "source": [
        "### Read the O'Reilly RSS plain text file articles into a corpus using the NLTK's PlaintextCorpusReader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmNkoVRZ4JIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feed = 'http://feeds.feedburner.com/oreilly/radar/atom'\n",
        "parsed = feedparser.parse(feed)\n",
        "posts = parsed.entries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3gQZzo8sQcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!\n",
        "!mkdir feed_oreilly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvOYMjkWsU8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tags_feed =['p']\n",
        "i=1\n",
        "for post in posts:\n",
        "        soup = BeautifulSoup(post['content'][0]['value'], 'html.parser')\n",
        "        text_list = [tag.get_text() for tag in soup.find_all(tags_feed)]\n",
        "        text = ' '.join(text_list)\n",
        "        #print(text)\n",
        "        filename = 'text_'+str(i)+'.txt'\n",
        "        i+=1\n",
        "\n",
        "        f = open('./feed_oreilly/'+filename,'w')\n",
        "        f.write(text)\n",
        "        f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFK1CEFhsagC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DOC_PATTERN = r'.*\\.txt'\n",
        "corpus_rss = PlaintextCorpusReader('feed_oreilly', DOC_PATTERN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJI6AbGA4JIg",
        "colab_type": "text"
      },
      "source": [
        "### Iterate through the fileids in the corpus, extract the raw text of each document, and store them in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TETqtk_Ds31u",
        "colab_type": "code",
        "outputId": "aa835f52-7c8a-4021-ade5-20a797e87583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL7Cjcby4JIi",
        "colab_type": "code",
        "outputId": "5d7bde15-5277-474d-f164-1a1eab5d14fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "doc_list = []\n",
        "for fileid in corpus_rss.fileids():\n",
        "    doc = corpus_rss.raw(fileid)\n",
        "    if len(doc)>0:\n",
        "        doc_list.append(doc)\n",
        "\n",
        "doc_list"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Look for the industry to become more stratified and specialized. The programming world will increasingly be split between highly trained professionals and people who don’t have a deep background but have a lot of experience building things. The former group builds tools, frameworks, languages, and platforms; the latter group connects things and builds websites, mobile apps, and the like. These two types of programmers have always existed, mixing fluidly. We just haven’t recognized the distinction, and that’s going to change. A good analogy is plumbing. If you need to install a toilet, you call a plumber: they know how to connect things together. There are jobs for people who design plumbing fixtures, but you wouldn’t want them working in your bathroom. Like reading, some people learn how to code with little training, and others don’t. But as with reading, we shouldn’t accept a world in which some people enter primary school programming-literate, and those that don’t have to wait until high school. We’ll need teachers who are trained in teaching programming—specifically, teaching programming in the early grades. We already have programming environments that are optimized for teaching children, including Scratch, Alice, and their relatives. And don’t discount the role gaming could play. Minecraft has unwittingly taught a generation of grade-schoolers how to program in Java. We also need to build bridges for people with great programming skills but without a deep computer science background—the plumbers—to enter the professional market. Some of those bridges exist already; they include the many boot camps and schools like General Assembly and Holberton. These are distinct from college degree-granting programs (the traditional computer science major) and serve a different purpose. They’re more like vocational education programs: They’re focused on practice, with minimal emphasis on theory. They’re about learning to program in a professional context—working with a web platform, a database, or even an AI platform—but not about developing those platforms or databases. They’re for those who say, “Why should I know how to program quicksort? If I want to sort something, I’ll call a library function.” That’s just fine, and we shouldn’t pretend that it isn’t. In contrast, CS majors should continue to be exposed to and work with theory and algorithms—not because they’re going to write their own quicksort but because we need people who can develop and implement new algorithms, and the best way to learn is to practice on algorithms we already understand. You don’t need to be good at math to program, but you do need math to push computing forward—particularly if you’re interested in data science or artificial intelligence. In “Hidden Technical Debt in Machine Learning Systems,” the authors—a group of researchers and engineers from Google—argue that machine learning is a relatively small part of any application. Much of the rest is wiring things together: building data pipelines, connecting the application to the serving infrastructure, providing for monitoring. It’s not glamorous, but it needs to be done, and done correctly. I’d bet that much more downtime results from bad plumbing than from bad implementations of ML algorithms. Rather than relying on our current crop of languages, I wonder whether or not there are better languages for this part of the enterprise. It’s long seemed strange to me that programming languages aren’t all that different from what they were in the 1960s and 1970s: line-oriented, alphanumeric texts, most often in fixed-width type. Functional languages date back to the 1950s, and the earliest roots of object-oriented programming aren’t much later. What would it mean to imagine other kinds of languages? Work is already being done on this front. There have been a surprising number of visual languages, which let users create programs using symbols or other graphic elements rather than text—although most have been unsuccessful. But even in popular languages like Scratch, we’re dealing with a simple mapping of visual objects to a traditional programming language: a “clamp” is a “loop,” a “variable” is a “box,” and so on. Is it possible to push even further beyond traditional programming languages? What would a programming language designed for plumbing look like? And would it give us better and more fruitful ways to think about the interconnections between systems? — Mike Loukides O’Reilly conferences combine expert insights from industry leaders with hands-on guidance about today’s most important technology topics. We hope you’ll join us at our upcoming events: O’Reilly Software Architecture Conference in New York, February 23-26, 2020 The O’Reilly Strata Data & AI Conference in San Jose, March 15-18, 2020',\n",
              " 'One of the most popular sections of the Next:Economy Newsletter is our periodic Deeper Reading feature, highlighting books of relevance to the ongoing transformation of the economy, the world of work, the costs of capitalism, and how business gets done. It’s nearly impossible to keep up with the recent spate of important, enlightening, and inspirational publications…and even if we could, we wouldn’t have the space to do justice to every book that provides a fresh perspective on how we think about work, entrepreneurship, capitalism, digital transformation, the construct of money, climate change, and other pertinent topics. To that end, we’ve put together a selection of books that comprise an engrossing Next Economy reading list for the year ahead. Last year, a reader of Tim O’Reilly’s book—WTF? What’s the Future and Why It’s Up to Us—pointed him to Marjorie Kelly’s 2001 book, The Divine Right of Capital, which he found revelatory. Kelly, cofounder of the Fifty by Fifty initiative to help build a more inclusive economy through employee ownership, came out with a new book this summer, cowritten with Ted Howard, cofounder and president of the Democracy Collaborative. The Making of a Democratic Economy: How to Build Prosperity for the Many, Not the Few addresses today’s systemic economic crisis of inequality and offers ideas for creating a more inclusive, democratic economy grounded in community, justice, and sustainability. Read it now on O’Reilly online learning. In his new book, The Great Reversal: How America Gave Up on Free Markets, economist Thomas Philippon, professor of finance at the NYU Stern School of Business, lays the blame for today’s US economic problems squarely at the feet of anticompetitive, monopolistic practices employed by the largest American corporations; he shows how these practices drive up prices for consumers while limiting choices and stifling investment, productivity, and wage growth, perpetuating a vicious cycle of inequality. Watch Philippon present the issues raised in his book, followed by a panel discussion, in this video from the Brookings Institution Global Economy and Development program. In a recent interview from the public radio show Marketplace, Philippon explains how “market concentration and low competition has become the new normal in America.” You can read this adapted excerpt in the Atlantic. (To understand the broader effects of the consolidation of corporations on politics and how it’s tearing society apart, check out Matt Stoller’s Goliath: The 100-Year War Between Monopoly Power and Democracy.) Renowned UC Berkeley economists Emmanuel Saez and Gabriel Zucman (who have helped US presidential candidate Elizabeth Warren devise her tax plan) detail the transformation that has turned the US tax code upside down (so that the working class pays more as the wealthiest Americans benefit from policies that lessen their load) and propose reforms that will rectify this shameful situation in The Triumph of Injustice: How the Rich Dodge Taxes and How to Make Them Pay. We’ve already shared this New York Times opinion piece by David Leonhardt, which clearly depicts how the tax rate on the rich has plummeted from 1950 to the present. The book’s authors also wrote a Times opinion piece filled with more straightforward data visualizations depicting the inequity of the US tax system: “How to Tax Our Way Back to Justice.” Newly minted Nobel laureates Abhijit V. Banerjee and Esther Duflo—a husband-and-wife team of economics professors at MIT—expand on their award-winning research in their new book, Good Economics for Hard Times. They pull apart conceptions perceived as common knowledge and dig into the facts about societal forces including inequality, migration, globalization, and taxation. Here’s a glowing review from the Guardian. Yet another New York Times opinion piece—this one by Nicholas Kristof, asking, “Should we soak the rich? You bet!”—draws on both this book and The Triumph of Injustice to make its argument. Harvard’s Joint Center for History and Economics hosted a launch conference for the book; watch the video here. You’ll find an extensive excerpt of the book when you scroll down here. Oxford economist Paul Collier delves into global societal divisions—between urbanites and rural dwellers, educated elites and the unskilled working class, and developed and emerging economies—and offers pragmatic, ethical ways to bridge these social, economic, and cultural chasms within a more compassionate, inclusive form of capitalism in his book The Future of Capitalism: Facing the New Anxieties. Bill Gates has given a great deal of thought to the book; in this review he agrees with most of its premises while arguing that Collier should expand his framework for rectifying capitalism’s excesses beyond the levels of global, national, company, and family to include community. In this review from the Financial Times, Martin Wolf despairs that a renewed sense of social obligation can counter the powerful forces of selfishness and greed. Collier discusses his book in this video from Oxford’s Blavatnik School of Government. James O’Toole, professor emeritus at the USC Marshall School of Business, founding director of the Neely Center for Ethical Leadership and Decision Making, and author of 17 books, turns his eye to business leaders who try to balance profitability and social purpose in his latest work, The Enlightened Capitalists: Cautionary Tales of Business Pioneers Who Tried to Do Well by Doing Good. As the subtitle forewarns, the impulse to build societal good into a corporation’s business practices has its perils; O’Toole offers case studies in the form of portraits of successful entrepreneurs who lost control of their enterprises to executives who did not share their belief that a company should do more than chase profits, including improving workers’ lives, advancing women in the workplace, and advocating for other social issues such as the environment. This review in the Financial Times points out that more recent business structure innovations—such as B Corporations, employee-owned enterprises, and cooperatives—offer better opportunities for compatibility of capitalism and corporate benevolence. In this Recode Decode interview by Kara Swisher, O’Toole tears into the Corporate Roundtable’s recent redefinition of the purpose of a corporation to care for more than short-term returns, expressing doubt that leaders of US corporations can “walk the walk.” The Enlightened Capitalists might be the perfect book to read as a prelude to recent autobiographies by two strikingly different business leaders who have stood up for social issues to the tune of millions of dollars. In It’s How We Play the Game: Build a Business. Take a Stand. Make a Difference., Ed Stack recounts how he built the DICK’S Sporting Goods empire from his father’s two Dick’s Bait and Tackle stores, always intending to be a force for good in the communities where the stores are located. Distressed by the rise in school shootings, Stack raised the age for in-store gun purchases to 21 and pulled semiautomatics from his shelves in the wake of the Parkland, Florida, attack, destroying $5 million in inventory and angering conservative lawmakers, the NRA, and a large part of his customer base while at the same time earning accolades for placing principles above profits. The Washington Post describes Stack’s bold move and its ramifications here, and Stack recounts his actions in the broader context of US gun control in this CBS News interview. Some people give discreetly and anonymously, but Salesforce founder, board chair, and co-CEO Marc Benioff is not that kind of guy. Benioff has famously put millions of dollars behind efforts to combat the homeless crisis in San Francisco, supporting legislation to raise taxes on large corporations to help the homeless (and getting into Twitter wars with his tech CEO peers who disagreed with Prop C) and, more recently, donating $30 million to launch the UCSF Benioff Homelessness and Housing Initiative. In Trailblazer: The Power of Business as the Greatest Platform for Change—written with Salesforce EVP of global strategic affairs Monica Langley—Benioff presents his vision for the role of business in making the world a better place and discusses how his company’s core values of trust, customer success, innovation, and equality; a commitment to giving back to society; and a focus on corporate culture have created a powerful competitive advantage. Benioff recounts his efforts to close Salesforce’s gender pay gap in this excerpt from Trailblazer in Wired. Andrew McAfee, cofounder and codirector of the MIT Initiative on the Digital Economy and a principal research scientist at the MIT Sloan School of Management, expresses a positive vision for the future propelled by the “four horsemen of the optimist”—effective capitalism, technological progress, public awareness of environmental issues, and responsive, effective government—in More from Less: The Surprising Story of How We Finally Learned to Prosper Using Fewer Resources—and What Happens Next. He asserts that free markets and tech innovation will enable people to prosper while government institutions and the free press will rein in rapacious greed and protect against social and environmental harm—beliefs that seem inconceivable at this point in US history, as citizens lose faith in crumbling institutions while a cynical administration fights to maintain power. Part of his optimism derives from the fact that the US standard of living has risen over the past two decades while American consumption of resources like water, metals, and building materials declines. McAfee’s concept of the decoupling of growth and environmental degradation—at least in prosperous, developed economies—comes into question in many reviews of his book; this post from the MIT Initiative on the Digital Economy blog addresses those qualms. Here’s an enlightening interview with McAfee by Russ Roberts in his EconTalk podcast. (The transcript is also available.) Political economist Ann Pettifor, cofounder and director of PRIME (Policy Research in Macroeconomics) and author of several books on economics, argues that financial markets, the economy, politics, and the ecosystem are inextricably bound together in her latest, The Case for the Green New Deal. She explains how governments can finance a Green New Deal without raising taxes in this Guardian story. Get a more detailed explanation of her rationale in this interview from New York magazine. As we call attention to an array of Next Economy-related books, we’d be remiss not to tout a few of our own. Here’s a sampling of recent releases, available for you to access right now on O’Reilly online learning. (If you’re not yet a member, try it free for 10 days.)',\n",
              " 'As we enter a new decade, we asked programming experts\\u2060—including several of our own O’Reilly authors and instructors\\u2060—for their thoughts on what’s in store for some established players and fast-growing languages. The biggest news this year in Python is that creator and “benevolent dictator for life” Guido van Rossum retired, leaving Python in the hands of the Python Steering Council. So far, it’s been a painless shift in power, which as Eric Matthes, author of Python Crash Course, argues, should come as no surprise, since “Guido has carried himself, and his role in the community, with such poise for so long.” 2020 will also see the end of support for Python 2.7, which will likely cause its share of headaches among holdouts. Meanwhile, Python continues to be the language of choice for data science. For Matthes, one exciting aspect of Python has been “the variety of interesting and critical projects that have come out of a community where diversity has so intentionally been built for so long.” Carol Willing, a member of the Python Steering Council and a core developer of CPython, also celebrates these projects—like the Binder service, which promotes reproducible research by creating an executable environment from your Jupyter Notebooks—particularly as they expand beyond their initial aims. Binder, she notes, “was widely used last year for teaching workshops and tutorials at many Python conferences.” Willing also offered a shout-out to the CircuitPython and Mu projects, asking, “Who doesn’t love hardware, blinking LEDs, sensors, and using Mu, a user-friendly editor that is fantastic for adults and kids?” It’s mostly good news on the Java front. Java Champion Ben Evans explains, “Once again, rumours of Java’s demise have proved to be little more than wishful thinking on the part of the platform’s detractors.” But it hasn’t all been smooth sailing. As we noted last year, the release of Java 11 in September 2018 brought a raft of new features, including many that give the release a significant, clear advantage for using containers. However, wide adoption of this latest release hasn’t followed suit, with more than 80% of developers still on Java 8, according to this JetBrains survey. Evans wonders, “Does this mean that people aren’t running Java in containers as much as we’re told they are? Or do people just not know about the benefits of 11 in containers?” Despite the slow rate of adoption, Java’s six-month release cadence has been trucking along—Java 12 dropped in March 2019, with Java 13 following in September. And according to Java Champion Trisha Gee, it’s really starting to show its value: Each release is quite small but predictable. And although they don’t all have exciting new language changes, you can see the language moving forward steadily. In addition, it enables this idea of preview features, which I think we saw working really well for switch expressions—developers got to try out the feature and give real feedback based on how it feels to work with, instead of feedback on abstract, conceptual ideas. In response, there was a small change to the syntax of switch expressions, which was possible due to it being a preview feature and not set in stone, in Java 13. Now this updated syntax is scheduled to be part of JDK 14 as a production-ready feature. 2019 brought another surprise when Oracle moved Java SE to a subscription-based model. But as Marc Loy, coauthor of Learning Java, fifth edition (now in early release), points out, “The Java community at large has approached this unfortunate change with increased enthusiasm for the OpenJDK.” As for the coming year, Evans suggests that 2020 will be about watching the 2019 trends play out: How close to a production version of Project Valhalla will we be? Will the incremental strategy of delivering pattern matching and algebraic data types (Project Amber) pay off? Will Quarkus bear out its promise and the faith of its early fans? Will 2020 be the year that Kotlin makes a significant beachhead beyond Android? These are exciting times—we’re in transition toward something new, and there’s a lot going on. Google announced in May 2019 that Kotlin is now its preferred language for Android app developers, boosting the language’s already strong adoption. Although many Android developers are still in the process of making the move to Kotlin, those who have already transitioned know the benefits it offers. Dawn and David Griffiths, authors of Head First Kotlin, share a few reasons behind Kotlin’s ascendance: For a language created by an IDE company, it’s no surprise that Kotlin has a healthy level of tooling support. The experimental DSL for code contracts gives developers the ability to provide guarantees about the ways that code behaves. Does your function have side effects? Is it guaranteed to return a non-null value? Code contracts allow you to make these promises, and the compiler can use them to loosen compile-time checks. The barriers between different Kotlin platforms are now also breaking down. The “expect”/”actual” qualifiers allow developers to more easily write code that is compatible across Java/Native/JS environments. And serialization support now means that it’s even easier to convert JSON data into Kotlin objects, and vice versa. Expect to see Kotlin continue its impressive growth—and not just in Android. Hadi Hariri, leader of the developer advocacy team at JetBrains, points to the success of Kotlin/Everywhere—a series of community-led events where you can learn the essentials and best practices of Kotlin in Android, Google Cloud Platform, and multiplatform development—as proof: “From May to November, we’ve managed to reach close to 30,000 people in 86 countries. KotlinConf sold out three years in a row with more than 1,700 attendees in 2019. This really shows, amongst other things, that interest and adoption of the language is growing.” When Gophers think back on 2019, they’ll likely remember the saga of the try proposal. Go developer and writer Jon Bodner explains: One of the most common complaints about Go is that error handling is too verbose. So in early June, the Go core developers proposed adding a new built-in function called try. A GitHub issue was opened to discuss this new feature. Within a month, there were nearly 800 comments, most of them negative. The people who were against the new feature felt that this change made code too “magical” and obscured the logic flow. After reviewing the feedback, the Go team marked the proposal as closed and rejected on July 16. What’s notable about this process wasn’t the failure of the feature but rather, as Bodner describes it, “the way the process happened: a feature was proposed, the discussion was respectful, but many felt that the change was inconsistent with Go’s style. In the end, the people who steward the language decided to respect the majority opinion. That’s what developers mean when they talk about community.” 2020 should bring more clarity to Go’s Contracts specification, better known as the Generics proposal. According to Bodner, “It looks like Go is going to implement generics using an approach that is a bit different from other languages, but which fits nicely into the idioms of Go.” It will hopefully allow Go to keep its idiomatic style while adding a feature that developers have found useful in other languages. We checked in with Jim Blandy, coauthor of Programming Rust, to see how his vision of Rust’s progress changed over the course of 2019. Last year, he noted that, “Rust has supported asynchronous programming in one form or another for a long time, but async functions provide a syntax for this sort of code that is a major improvement over what Rust has had before.” Did his hope for improvements to the Rust syntax come to fruition? Yes, eventually: Blandy explained that async/await syntax didn’t become stable until version 1.39, which was released November 7, 2019. “Originally, we were hoping async/await syntax could be part of the 2018 edition of Rust, but it took longer to get things right.” Still, he has high hopes for what async will mean for Rust in 2020: “Integrating async into the language lets the borrow checker understand what you’re doing, so asynchronous code can look like idiomatic Rust.” And as Blandy points out, the Rust ecosystem is acting quickly to take advantage of the language’s new expressiveness. The Rust community is also excited about WebAssembly, which this year became a theoretical replacement to C/FFI for ecosystems that need portable, high-performance modules. And as Rust expert Nathan Stocks notes, “You get light sandboxing as well!” What impressed Stocks most was “how much of the theory had been prototyped and demonstrated successfully.” I had previously thought of WebAssembly purely as a compilation target to run code from non-JS languages in the browser. The addition of the ability to consume web assembly from any language outside the browser was mind-bending. The biggest stories in Swift last year were the releases of SwiftUI, Apple’s newest framework for designing user interfaces across all Apple devices, and Swift for TensorFlow, a platform for deep learning and differentiable programming integrating Google’s TensorFlow framework with Swift. SwiftUI, as Timirah James explains, “has already gained so much traction amongst developers (rightfully so) with its declarative nature and is already being seen as a possible future successor to UIKit.” As for Swift for TensorFlow, Paris Buttfield-Addison calls it “a radical new use for Swift.” He explains, “Swift has always been a great app development and systems programming language, and is a great up-and-coming web and back-end development language, but now, with Swift for TensorFlow, it’s a powerful ML framework, too.” Here’s why: Swift for TensorFlow is developed by a team that includes the original creator of Swift, Chris Lattner, and provides (or will, when it’s done) everything you need for machine learning and numerical computing. Most surprisingly is the full first-class support for differentiable programming with automatic differentiation, which is made possible by Swift’s underlying compiler framework and design. Full in-language differentiable programming will make a whole collection of previously impossible things possible: the best example is being able to use a standard programming debugger to step through backpropagation and debug derivatives when you’re building a neural network. Swift for TensorFlow also brings full Python support to Swift, allowing data scientists to mix and match the useful and familiar Python frameworks they need, with clean expressive Swift code. Looking ahead, both James and Buttfield-Addison are excited to see the new directions Swift takes, with James pointing to “Swift adoption across different communities and stacks beyond mobile, especially in the serverless realm,” and Buttfield-Addison calling out “amazing web development frameworks, like Kitura, and all sorts of amazing frameworks for niche areas…such as SwiftPlot, which is a Swift native version of the ubiquitous Matplotlib from Python.” Change is inevitable, and as programming languages continue to lean in to optimization for new trends in the cloud, microservices, big data, and machine learning, each language and its ecosystem will continue to adapt in its own unique way. Big releases may be on the horizon in 2020 for certain languages—C++20 will be released this summer and Scala 3.0 is expected in late 2020—but what’s clear is even the smallest changes can cause huge waves in the daily lives of programmers.',\n",
              " \"Roger Magoulas recently sat down with Rob Thomas and Tim O’Reilly to discuss Thomas’s AI framework called the AI Ladder, which, according to his recent paper, is a framework that describes “the increasing levels of analytic sophistication that lead to, and buttress, a thriving AI environment.” Thomas notes both in his paper and in a recent keynote discussion he had with O’Reilly that “there is no AI without IA [information architecture].” In this interview, Thomas and O’Reilly delve deeper into their conversation, outlining the “rungs” of the ladder and some of the big-picture opportunities and consequences of AI in real-time business environments. Here are some highlights: Thomas notes that the framework embraces an iterative approach: “I would call it a builder’s market, where people have got to get their hands on the tools and go try things. It’s not about building a nine month strategic plan, and then building a huge team. It’s about picking a problem—making better predictions or automating something, or trying to optimize a business process—and go give it a try.” (01:20) O’Reilly expands on the iterative concept, noting that working iteratively helps get beyond the hype and the “all-or-nothing” perspective AI hype fosters. “AI has had so much hype attached to it,” he explains, “that everybody comes away with a sort of binary: it’s either the complete self-driving car that didn’t really work or is not doing as well as they thought, so even the big dudes can’t do it.\\xa0 So therefore, it’s over-hyped, or it’s nothing. And part of what the AI Ladder gets across is, yes, there are very futuristic projects that tend to represent AI in people’s minds, but there’s actually a series of steps used to get there. So companies think, ‘Well, what’s that one big win that we could have like the one that Google’s working on?’ And that’s not the right way to think about this. You want to get up there, but you have to start at the bottom of the ladder, and you have to do a bunch of work to get ready, and then you do a bunch of small projects and you gradually build your competency, rather than simply saying, ‘I’ve got to get some of that AI magic, so I’ll go to a vendor who promises to do something that sounds magical to me.'” (01:54) The work, Thomas says, starts with the “lingua franca of the AI world,” which Thomas and O’Reilly list as such languages as Python, TensorFlow, and PyTorch. “This is computer science,” says Thomas, “and it’s computer science disguised as hard work. You better roll up your sleeves. … I think it’s hard for a lot of people to get their heads around the idea that whatever we’re doing today, we’re probably going to be doing something different in six to 12 months. So, it will take constant learning to do this well.” (03:22) Communication is going to be key, Thomas notes, which is going to require a way to unlock normal human communication—in written form, spoken form, structured and unstructured text, etc.—to get to the real insights. “That’s why I say NLP is ultimately going to become this nervous system,” Thomas says, “where if you can do that very well; it’s going to make a big difference. And there are industry benchmarks on this. The newest one’s called SuperGLUE. … So we’re getting almost to a human level in NLP, and these benchmarks will continue to move the bar, which is good because it challenges us to be better. (11:12) Thomas says his company encourages clients to take steps toward AI adoption because major factors are coming together to make this an opportune time to get on board early. “This is finally becoming a board-level topic for companies I interact with,” he said. “Just look at the economics: $16 trillion of GDP is expected to be accrued from AI between now and 2030. It’s hard to overlook numbers that big. Let’s say that’s off by 50%; it’s still a big number. So, there’s an economic piece. Adoption today—meaning companies that have seriously done something with AI—depending on who you believe, is somewhere between 4-8%. You take those two things—the biggest economic opportunity any of us will ever see in our lives, and very low adoption—that’s a pretty good opportunity to step into the moment and do something as a company.” (14:45) It’s important for companies to innovate in these areas, too, O’Reilly notes, because the problems we’re going to face in the coming decades are going to require it. “The thing I get most excited about is that we are growing our data universe, and we have to grow our “understanding universe” as well. You think about things like handheld DNA sensors. We had a demonstration of this device at our Science Foo Camp. They were using it to look at a virus that was affecting cassava roots in Africa–they literally were doing handheld gene sequencing in the field. Think about how compute power is going out to an edge like that, and you start adding up all of those edges–we had a presentation this morning, for example, about how a new crop disease or plague of insects, or whatever, in some part of the world could have an effect on commodity prices worldwide. That’s the kind of stuff we’re going to be building systems for so we’re increasingly able to respond in real time. When I look at the arc of history, the problems we’re going to be hitting in the 21st century are so large that we’ll need all the help we can get.” (15:36)\",\n",
              " 'Although blockchain technology is still in its early days, momentum has been building in the enterprise. While there has been much focus on blockchains in banking and payments, its impact has already extended far beyond finance. Here is a glimpse of just some of this work and the kind of intense learning that can be leveraged across industries. With its high bar for data privacy and security, lessons learned in health care could inform development and practice in government and other regulation-heavy industries. Regulatory requirements make health care one of the more challenging industries to tackle, so change will take time to trickle down to patients in a way they really notice. But development has been underway for years already. Visionaries are hoping to leverage blockchains to provide a secure, universal data sharing infrastructure that can help solve decades-long problems endemic to the health care system. Because providers today don’t have a way to securely share data, there is hope that blockchain-driven collaboration could ultimately increase accuracy of diagnosis and coordination of treatment. For example, MedRec is a system in development at MIT that seeks to make it possible for patients to manage their own records, giving permission to different providers to access and update data. The system is being tested with anonymized data, and could be used, for example, as a trusted central repository for vaccination, even if those vaccines were given by different providers. Gem and Tierion are startups working different aspects of data storage, verification and sharing (both partnered with Philips Healthcare), while Hu-manity.co partnered with IBM to give patients more control of their data and enable them to profit directly from sharing their data. MetLife can now pay claims instantly to expectant mothers who test positive for gestational diabetes—while this represents just a small sliver of business, it is easy to project how this learning could be extensible to other populations. Pioneers are working to increase liquidity, ease administrative burden, and open up the possibility of new business models in real estate. Their work could inspire innovations in other spaces with similar characteristics. They are using the technology to record, track, and transfer land titles, deed, and liens, and to facilitate payments, leasing, and sales. They are even exploring new ways to fractionalize real estate through tokenization (programmable representations of value). The Swedish land registry authority, Lantmäteriet, has, with a diverse team of partners, been testing land registry on a private blockchain. Today, it can take months from the signing of a contract to register a sale, even though Lantmäteriet is already paperless and highly digitized. The registry is automating transactions on a blockchain, enabling buyers and sellers to digitally sign a bill of sale, with signatures verified automatically instead of at an agent’s office. When a land title changes hands, it would be verified via the blockchain, and recorded again. It’s still a long way until a blockchain-driven registry is widely available in Sweden, but estimates put taxpayers’ potential savings at over $100 million a year. In the U.S., the city of South Burlington in Vermont has also run pilots with land title and registry on a blockchain. Platforms have emerged to facilitate the tokenization of real estate, which can help make an illiquid asset more liquid. Buyers purchase tokens, each representing a fraction of the asset. Last year, the owners of the St. Regis Aspen Resort in Colorado raised $18 million in this way, with each token valued at $1 USD. A great deal of investment in blockchain technology has poured in from consumer packaged goods companies attracted by the promise of more supply chain transparency. Their challenging work—how, for example, do you digitize a strawberry?—could lay important groundwork for any industry looking to achieve more visibility of goods moving through their ecosystem. More than 100 companies are involved with IBM’s Food Trust network, including many consumer packaged goods companies and grocery retailers. Consumers, retailers, and food safety organizations are demanding more transparency, and blockchain is looking to be a promising solution for this complex ecosystem. Bumble Bee Foods is using a blockchain to achieve transparency in its supply chain all the way from the moment a tuna is caught by fishermen to arriving on grocery store shelves. The company is aiming to demonstrate provenance of its product to influence consumer preference and even pricing. Nestle has been testing blockchains in more than 10 projects, including IBM Food Trust, where it is using the technology to track provenance of ingredients in products like Gerber baby food. Anheuser-Busch InBev is exploring a range of pilots. The brewing company is giving some consumers an option to upload their driver’s license information to a blockchain, which verifies age so they can then buy beer at a vending machine simply by scanning their phone. Anheuser-Busch InBev also has a partnership with BanQu that uses a blockchain to facilitate payments to farmers who don’t have bank accounts. In fast growing markets, like Africa, this could enable the company to meet demand more efficiently by making it possible to work with more farmers. Digital advertising depends on a complex ecosystem of advertisers, publishers, and middlemen, and is plagued by widespread fraud. Blockchain provides hope for smarter, more secure advertising that generates higher yields. Entrepreneurs and intrapreneurs are exploring many angles, including using blockchains for higher quality data, targeting, and reducing costs. This work could inform a range of industries that are operating with large ecosystems characterized by fraud or high-toll intermediaries. For example, Comcast and competitors Viacom and Spectrum Reach have partnered to launch Blockgraph, which seeks to enable more precise targeting without disclosing viewers’ personal information to advertisers. Brave is a new browser that sees to attack each layer of inefficiency in the browser ecosystem. This new model uses a blockchain to eliminate the middlemen that today take more than half the revenue. Instead of going to Google or Facebook, for example, advertisers will list directly onto Brave’s blockchain-based browser. Both advertisers and publishers get more value, and consumers get fewer but better targeted ads. The application of blockchain in the insurance industry is an area of great exploration. The technology is being targeted as a way to detect fraud, improve the efficiency of claims processing, and simplify the flow of data and payments between insurers and reinsurers. But it is also being explored as a key mechanism for innovative approaches to insurance. Blockchains (along with a new flood of IoT-driven data that can be used to calculate risk and monitor assets) could make it easier for insurers to offer granular protection that flexes and changes with individual needs. It drives costs down and enables unique identifiers, immutable tracking, and contracts that can be executed without a human—making it possible to make insurance work for smaller and smaller things or periods of time. This may also make it easier for non-insurers to bundle insurance with their products and services, creating new, innovative packaging that drives customer preference in a broad range of industries. Insurance giant Allianz SE has been testing a range of blockchain applications. For example, through a joint venture, the company is testing flight-delay insurance that could be automatically initiated as soon as a flight is delayed by the insured delay time. It is also involved with a project to speed insurance payments to health providers. State Farm is working on a blockchain to speed the process in which insurers seek partial reimbursements for claims from other insurers involved with the claim, today a painful process with high administrative costs. These industries provide just a glimpse of the kinds of projects that are building momentum. Just about every industry has both established and disruptive players looking to use blockchain technology to evolve the way business is done. According to Deloitte in their 2019 Global Blockchain Survey, “the question for executives is no longer, ‘will blockchain work?’ but, ‘how can we make blockchain work for us?”. While many enterprise projects tend to be lodged in the deep back office, they create a foundation that could be leveraged to one day impact the way we work and play. As blockchain savvy teams around the globe build and gather allies, projects could build, over time, to something pervasive and difficult to outrun.',\n",
              " 'For all the excitement about machine learning (ML), there are serious impediments to its widespread adoption. Not least is the broadening realization that ML models can fail. And that’s why model debugging, the art and science of understanding and fixing problems in ML models, is so critical to the future of ML. Without being able to troubleshoot models when they underperform or misbehave, organizations simply won’t be able to adopt and deploy ML at scale. Because all ML models make mistakes, everyone who cares about ML should also care about model debugging.[1] This includes C-suite executives, front-line data scientists, and risk, legal, and compliance personnel. This article is meant to be a short, relatively technical primer on what model debugging is, what you should know about it, and the basics of how to debug models in practice. These recommendations are based on our experience, both as a data scientist and as a lawyer, focused on managing the risks of deploying ML. Sometimes ML models are just plain wrong, but sometimes they’re wrong and socially discriminatory, or hacked, or simply unethical.[2],[3],[4] Current model assessment techniques, like cross-validation or receiver operator characteristic (ROC) and lift curves, simply don’t tell us about all the nasty things that can happen when ML models are deployed as part of large, complex, public-facing IT systems.[5] That’s where model debugging comes in. Model debugging is an emergent discipline focused on finding and fixing problems in ML systems. In addition to newer innovations, the practice borrows from model risk management, traditional model diagnostics, and software testing. Model debugging attempts to test ML models like code (because they are usually code) and to probe sophisticated ML response functions and decision boundaries to detect and correct accuracy, fairness, security, and other problems in ML systems.[6] Debugging may focus on a variety of failure modes (i.e., a lot can go wrong with ML models), including: The best way to prevent and prepare for these kinds of problems is model debugging. We’ll review methods for debugging below. There are at least four major ways for data scientists to find bugs in ML models: sensitivity analysis, residual analysis, benchmark models, and ML security audits. While our analysis of each method may appear technical, we believe that understanding the tools available, and how to use them, is critical for all risk management teams. Anyone, of any technical ability, should be able to at least think about using model debugging techniques. Sensitivity analysis, sometimes called what-if? analysis, is a mainstay of model debugging. It’s a very simple and powerful idea: simulate data that you find interesting and see what a model predicts for that data. Because ML models can react in very surprising ways to data they’ve never seen before, it’s safest to test all of your ML models with sensitivity analysis.[9] While it is relatively straightforward to conduct sensitivity analysis without a formal framework, the What-If Tool is a great way to start playing with certain kinds of models in the TensorFlow family. More structured approaches to sensitivity analysis include: Residual analysis is another well-known family of model debugging techniques. Residuals are a numeric measurement of model errors, essentially the difference between the model’s prediction and the known true outcome. Small residuals usually mean a model is right, and large residuals usually mean a model is wrong. Residual plots place input data and predictions into a two-dimensional visualization where influential outliers, data-quality problems, and other types of bugs often become plainly visible. The main drawback of residual analysis is that to calculate residuals, true outcomes are needed. That means it can be hard to work with residuals in some real-time model monitoring settings, but residual analysis should always be doable at model training time. Like in Figure 2, many discrimination detection techniques consider model errors as well, especially across different demographic groups. This basic bias detection exercise is sometimes called disparate impact analysis.[10] The Gender Shades line of research is a great example of how analyzing errors across demographic groups is necessary for models that affect people.[3] There are a myriad of other tools available for discrimination detection. To learn more about testing ML models for discrimination, check out packages like aequitas, AIF360, Themis, and, more generally, the content created by the Fairness, Accountability, and Transparency in ML (FATML) community.[11] Benchmark models are trusted, simple, or interpretable models to which ML models can be compared. It’s always a good idea to check that a new complex ML model does actually outperform a simpler benchmark model. Once an ML model passes this benchmark test, the benchmark model can serve as a solid debugging tool. Benchmark models can be used to ask questions like: “what predictions did my ML model get wrong that my benchmark model got right, and why?” Comparing benchmark model and ML model predictions in real time can also help to catch accuracy, fairness, or security anomalies as they occur. There are several known attacks against machine learning models that can lead to altered, harmful model outcomes or to exposure of sensitive training data.[8],[12] Again, traditional model assessment measures don’t tell us much about whether a model is secure. In addition to other debugging steps, it may be prudent to add some or all of the known ML attacks into any white-hat hacking exercises or red-team audits an organization is already conducting. So you’ve implemented some of the systematic ways to find accuracy, fairness, and security problems in ML-based systems that we’ve discussed. You’ve even discovered a few problems with your ML model. What can you do? That’s where remediation strategies come in. We discuss seven remediation strategies below. ML models learn from data to become accurate, and ML models require data that’s truly representative of the entire problem space being modeled. If a model is failing, adding representative data into its training set can work wonders. Data augmentation can be a remediation strategy for discrimination in ML models, too. One major source of discrimination in ML is demographically unbalanced training data. If a model is going to be used on all kinds of people, it’s best to ensure the training data has a representative distribution of all kinds of people as well. The debugging techniques we propose should work on almost any kind of ML-based predictive model. But they will be easier to execute on interpretable models or with explainable ML. For this reason, and others, we recommend interpretable and explainable ML for high-stakes use cases. Luckily, technological progress has been made toward this end in recent years. There are a lot of options for interpretable and accurate ML models and a lot of ways to explain and describe them.[13] Some ML models are designed to be interpretable so it is possible to understand how they work. Some of these models, like variants of decision trees or GA2M (i.e., explainable boosting machines) can be directly editable by human users. If there’s something objectionable in the inner workings of a GA2M model, it’s not very hard to find it and change the final model equation to get rid of it. Other models might not be as easy to edit as GA2M or decision trees, but if they generate human-readable computer code, they can be edited. Model assertions can improve or override model predictions in real time.[14] Model assertions are business rules that act on model predictions themselves. Examples could include checking the age of a customer to whom a model recommends advertising alcoholic beverages, or checking for large prepayments for a prediction that says a high net worth individual is about to default. There are a lot of ways to fix discrimination in ML models. Many non-technological solutions involve promoting a diversity of expertise and experience on data science teams, and ensuring diverse intellects are involved in all stages of model building.[15] Organizations should, if possible, require that all important data science projects include personnel with expertise in ethics, privacy, social sciences, or other related disciplines. From a technical perspective, discrimination remediation methods fall into three major buckets: data pre-processing, model training and selection, and prediction post-processing. For pre-processing, careful feature selection, and sampling and reweighing rows to minimize discrimination in training data can be helpful. For model training and selection, we recommend considering fairness metrics when selecting hyperparameters and decision cutoff thresholds. This may also involve training fair models directly by learning fair representations (LFR) and adversarial debiasing in AIF360, or using dual objective functions that consider both accuracy and fairness metrics. Last, for prediction post-processing, changing model predictions after training, like reject-option classification in AIF360 or Themis ML, can also help to reduce unwanted bias. Model debugging is not a one-and-done task. The accuracy, fairness, or security characteristics of ML models are not static. They can change significantly over time based on the model’s operating environment. We recommend monitoring ML models for accuracy, fairness, and security problems at regular time intervals once they are deployed. Strange, anomalous input and prediction values are always worrisome in ML, and can be indicative of an adversarial attack on an ML model. Luckily, anomalous inputs and predictions can be caught and corrected in real time using a variety of tools and techniques: data integrity constraints on input data streams, statistical process control methodologies on inputs and predictions, anomaly detection through autoencoders and isolation forests, and also by comparing ML predictions to benchmark model predictions. Everyone wants trustworthy ML models. And that means that as ML is more widely adopted, the importance of model debugging will only increase over time. That holds true for everyone from Kagglers to front-line data scientists to legal and risk management personnel and for ML consumers and decision subjects. Those interested in more details can dig deeper into the code on GitHub used to create the examples in this post.[16] Or, you can learn more about model debugging in the ML research community by checking out the 2019 International Conference on Learning Representations (ICLR) Debugging Machine Learning Models workshop proceedings.[17] Hopefully some of these techniques will work for you and your team. If so, have fun debugging! \\n[1] “All models are wrong, but some are useful.” — George Box, Statistician (1919 – 2013) \\n[2] The Security of Machine Learning \\n[3] How well do IBM, Microsoft, and Face++ AI services guess the gender of a face? \\n[4] Fairwashing: The Risk of Rationalization, How Can We Fool LIME and SHAP? Adversarial Attacks on Post-hoc Explanation Methods \\n[5] Machine Learning: The High Interest Credit Card of Technical Debt \\n[6] See: Testing and Debugging Machine Learning Models \\n[7] See: A Viral Tweet Accused Apple’s New Credit Card of Being ‘Sexist.’ Now New York State Regulators Are Investigating or Why Addressing Ethical Questions in AI Will Benefit Organizations \\n[8] Warning Signs: The Future of Security and Privacy in an Age of Machine Learning \\n[9] See: Teach/Me Data Analysis \\n[10] For a technical analysis, see Certifying and Removing Disparate Impact. \\n[11] Fairness, Accountability, and Transparency in Machine Learning \\n[12] Proposals for Model Vulnerability and Security \\n[13] An Introduction to Machine Learning Interpretability \\n[14] Model Assertions for Debugging Machine Learning \\n[15] Beyond Explainability: A Practical Guide to Managing Risk in Machine Learning Models \\n[16] Interpretable Machine Learning with Python \\n[17] Debugging Machine Learning Models',\n",
              " 'Roughly a year ago, we wrote “What machine learning means for software development.” In that article, we talked about Andrej Karpathy’s concept of Software 2.0. Karpathy argues that we’re at the beginning of a profound change in the way software is developed. Up until now, we’ve built systems by carefully and painstakingly telling systems exactly what to do, instruction by instruction. The process is slow, tedious, and error-prone; most of us have spent days staring at a program that should work, but doesn’t. And most of us have been surprised when some program that has been reliable for some time suddenly screws up at some slightly unexpected input. The last bug is always the one you find next; if someone hasn’t already said that, someone should have. Karpathy suggests something radically different: with machine learning, we can stop thinking of programming as writing a step of instructions in a programming language like C or Java or Python. Instead, we can program by example. We can collect many examples of what we want the program to do and what not to do (examples of correct and incorrect behavior), label them appropriately, and train a model to perform correctly on new inputs. In short, we can use machine learning to automate software development itself. It’s time to evaluate what has happened in the year since we wrote that article. Are we seeing the first steps toward the adoption of Software 2.0? Yes, but so far, they’re only small steps. Most companies don’t have the AI expertise to implement Karpathy’s vision. Traditional programming is well understood. Training models isn’t well understood yet, at least not within companies that haven’t already invested significantly in technology (in general) or AI (in particular). Nor are building data pipelines and deploying ML systems well understood. The companies that are systematizing how they develop ML and AI applications are companies that already have advanced AI practices. That doesn’t mean we aren’t seeing tools to automate various aspects of software engineering and data science. Those tools are starting to appear, particularly for building deep learning models. We’re seeing continued adoption of tools like AWS’ Sagemaker and Google’s AutoML. AutoML Vision allows you to build models without having to code; we’re also seeing code-free model building from startups like MLJAR and Lobe, and tools focused on computer vision, such as Platform.ai and Matroid. A sign that companies are scaling up their usage of ML and AI is that we are seeing the rise of data platforms aimed at accelerating the development and deployment of ML within companies that are growing teams focused on machine learning and AI. Several leaders in AI have described platforms they’ve built internally (such as Uber’s Michelangelo, Facebook’s FBLearner, Twitter’s Cortex, and Apple’s Overton); these companies are having an influence on other companies that are starting to build their own tools. Companies like Databricks are building Software as a Service (SaaS) or on-premises tools for companies that aren’t ready to build their own platform. We’ve also seen (and featured at O’Reilly’s AI Conference) Snorkel, an ML-driven tool for automated data labeling and synthetic data generation. HoloClean, another tool developed by researchers from Stanford, Waterloo, and Wisconsin, undertakes automatic error detection and repair. As Chris Ré said at our conference, we’ve made a lot of progress in automating data collection and model generation; but labeling and cleaning data have stubbornly resisted automation. At O’Reilly’s AI Conference in Beijing, Tim Kraska of MIT discussed how machine learning models have out-performed standard, well-known algorithms for database optimization, disk storage optimization, basic data structures, and even process scheduling. The hand-crafted algorithms you learned in school may cease to be relevant, because AI can do better. Rather than learning about sorting and indexing, the next generation of programmers may learn how to apply machine learning to these problems. One of the most suggestive projects we’ve seen has been RISE Lab’s AutoPandas. Given a set of inputs, and the outputs those inputs should produce, AutoPandas generates a program based on those inputs and outputs. This “programming by example” is an exciting step toward Software 2.0. What are the biggest obstacles to adoption? The same set of problems that AI and ML are facing everywhere else (and that, honestly, every new technology faces): lack of skilled people, trouble finding the right use cases, and the difficulty of finding data. That’s one reason Software 2.0 is having the greatest influence on data science: that’s where the skilled people are. Those are the same people who know how to collect and preprocess data, and who know how to define problems that can realistically be solved by ML systems. With AutoPandas, and automated tools for optimizing database queries, we’re just starting to see AI tools that are aimed at software developers. Machine learning also comes with certain risks, and many businesses may not be willing to accept those risks. Traditional programming is by no means risk-free, but at least those risks are familiar. Machine learning raises the question of explainability. You may not be able to explain why your software does what it does, and there are many application domains (for example, medicine and law) where explainability is essential. Reliability is also a problem: it’s not possible to build a machine learning system that is 100% accurate. If you train a system to manage inventory, how many of that system’s decisions will be incorrect? It might make fewer errors than a human, but we’re more comfortable with the kinds of errors humans make. We’re only starting to understand the security implications of machine learning, and wherever data is involved, privacy questions are almost certain to follow. Understanding and addressing the risks of ML and AI will require cross-functional teams; these teams need to encompass not only people with different kinds of expertise (security, privacy, compliance, ethics, design, and domain expertise), but also people from different social and cultural backgrounds. Risks that one socio-cultural group accepts without thinking twice are often completely unacceptable to those with different backgrounds; think, for example, what the use of face identification means to people in Hong Kong. These problems, though, are solvable. Model governance, model operations, data provenance, and data lineage are becoming hot topics for people and organizations that are implementing AI solutions. Understanding where your data comes from and how it has been modified, along with understanding how your models are evolving over time, is a critical step in addressing safety. Governance and provenance will become even more important as data use becomes subject to regulation; and we’re starting to see data-driven businesses follow the lead of companies in highly regulated industries, such as banking and health care. We are at the edge of a revolution in how we build software. How far will that revolution extend? We don’t know; it’s hard to imagine AI systems designing good user interfaces for humans–though once designed, it’s easy to imagine AI building those interfaces. Nor is it easy to imagine AI systems designing good APIs for programmatic access to applications. But it’s clear that AI can and will have a big influence on how we develop software. Perhaps the biggest change won’t be a reduction in the need for programmers, but in freeing programmers to think more about what we’re doing, and why. What are the right problems to solve? How do we create software that’s useful to everyone? That’s ultimately a more important problem than building yet another online shopping app. And if Software 2.0 lets us pay more attention to those questions, it will be a revolution that’s truly worthwhile.',\n",
              " 'In a fast-paced digital world, it is tempting to suppose that deploying the latest technology is the lynchpin to competitive advantage. Extremely powerful digital technologies are now accessible to most companies—meaning you must find new ways to distinguish yourself in a crowded field. One very effective method for unearthing strategies that differentiate your organization from everyone else is to leverage your people as a competitive advantage. “Using people,” in this context, means placing an emphasis on any full-time employee, contractor, partner, supplier, gig worker, etc., who is part of your extended ecosystem of strategy and delivery. These are the groups and individuals who provide the fuel to make your technology engine work. You will find them interspersed inside and outside your organization, and it is your task to harness their collective power to build a coalition of support for your business. If you do this well, you can use ecosystem engagement as a way to build stickiness between your organization and employees. As we progress through this latest digital era, we are starting to see more strength in numbers—particularly from unexpected partnerships that provide different perspectives and allow ideas and innovation to originate from a variety of sources. By embracing cognitive diversity, these partnerships thrive because they’ve learned how to tap into a variety of players in order to design and deliver a compelling solution. Each time you tap into it, your people ecosystem presents a blank canvas of potential innovation, and capitalizing on their insight and adaptability creates new opportunities for unique advantage. Organizations that struggle to keep or fully engage their people ecosystem may find themselves facing the quandary of having people who once worked with them or for them easily becoming foes. History shows us that not recognizing the latent talent in your untapped, disengaged employees creates a hazard for your organization: a cohort of employees who understand your strategic priorities and what you value and use that knowledge to compete, providing new services aimed at dissatisfied customers looking for new options. Armed with the quick and easy tools that make spinning up a startup or a competitive initiative effortless, former employees can quickly wreak havoc on your best-laid plans—something that it used to take a much larger organization to accomplish. What can you do? Go on the offense, pursuing a strong people-based competitive advantage to build a vibrant ecosystem of employees and partners who feel valued and aligned with your organizational goals. A significant challenge you might face when you are looking to strengthen your people ecosystem is the temptation to continue operating within your traditional mode of working, and within your traditional types of relationships. Your old ways of working required one level of engagement, but to exploit the power of the digital economy, you have to bring new players into the ecosystem and be far more intentional about the way you interact with them to use their capabilities. As you do this, you will quickly find that what it takes to make one group or person happy is not what it takes to make another group or person happy, and you will constantly negotiate outcomes to ensure everyone feels like they are in a win-win scenario. The digital economy moves at a frenetic pace and causes massive changes in personal and societal expectations. Solutions that worked just a few years ago will no longer keep customers and partners happy. That’s why it’s vital that you maintain an environment where people feel they are in a win-win relationship and they see your success as something that’s good for them. Leaders who assume they have good relationships with freelancers, contractors, or suppliers can quickly find themselves struggling with the same concerns vocalized by disengaged employees, and this type of disconnect can dramatically impact your level of output. You lose a great deal of innovation capacity when you can no longer count on people to give the extra effort that comes because they are autonomously motivated to succeed. To keep everyone happy and excited at the same time, you will need an agile engagement approach. That means the painstaking work you do today to build a high degree of engagement with your people may have to be revisited often to keep them invested in your organization. If you want to develop a strong, competitive advantage built on the expertise of people in your ecosystem, you should focus on five key strategies: As you focus on these areas, you will naturally see new and different capabilities emerge. Recognize these capabilities in your people not just for what they can do for you now, but for what they can do for you in the future, and then strategically position them so they can help you transform the business. Placing people in flexible roles helps them come up with autonomous innovation because the deeper they get into technology, the content, or their knowledge of the business, the more they uncover opportunities for you to get better, to modernize, to offer new solutions, or to have the next big thing that might transform the industry. Instead of looking at your people solely as tools to fulfill your current work, look at them as opportunities to uncover new jobs, expand your offerings, design new partnerships, discover a new mission, and recognize new customers. By doing that, you will have a talent advantage that cannot be duplicated.',\n",
              " 'For the past year, 5G cell technology has generated a lot of excitement–and a lot of hype. The specifications are impressive: 5G will provide a peak data rate of up to 20 Gbps (with 100 Mbps of “user experienced data rate”) to mobile devices: cell phones, smart cars, and a lot of devices that haven’t been invented yet. It’s difficult to imagine mobile applications that will require that much data, and 5G’s proponents seem willing to promise just about anything. What will 5G mean in practice? If it’s going to make any real difference, we’ll need to think that through. The most obvious change 5G might bring about isn’t to cell phones but to local networks, whether at home or in the office. Back in the 1980s, Nicholas Negroponte said everything wired will become wireless, and everything wireless will become wired. What happens to “last mile” connectivity, which seems to be stuck somewhere around 50 Mbps for homes and several times that for business service? It would be great to have an alternative to the local cable monopoly for high-bandwidth connectivity. We were supposed to have fiber to the home by now. I don’t, do you? High-speed networks through 5G may represent the next generation of cord cutting. Can 5G replace wired broadband, allowing one wireless service for home and mobile connectivity? I don’t need more bandwidth for video conferences or movies, but I would like to be able to download operating system updates and other large items in seconds rather than minutes. Anyone who has ever built a Docker container has experienced “now we wait for some giant things to download and be uncompressed.” Those waits can be significant, even if you’re on a corporate network. They could disappear. Rural connectivity is a persistent problem; many rural users (and some urban users) are still limited to dial-up speeds. Although the industry claims that 5G will provide better connectivity for rural areas, I’m skeptical. Because 5G uses higher frequencies than 4G, and higher frequencies are more subject to path loss, 5G cells have to be smaller than 4G/LTE cells. If carriers won’t build cell towers for current technology, they aren’t likely to build even more towers for 5G. I suspect rural communities will be left in the dark–again. As far as mobile and embedded devices go, I don’t see why I need a gigabit on my phone, except perhaps to serve as a Wi-Fi hub when traveling. Phones are a painful way to watch movies–more about that later. 5G enthusiasts frequently say it’s an enabling technology for autonomous vehicles (AV), which will need high bandwidth to download maps and images, and perhaps even to communicate with each other: AV heaven is a world in which all vehicles are autonomous and can therefore collaboratively plan traffic. That may well require 5G–though again, I wonder who is going to make the investment in building out rural networks. Autonomous vehicles that only work in urban or suburban areas are less useful. For applications like communication between AVs, latency–how long it takes to get a response–is more likely to be a bigger limitation than raw bandwidth, and is subject to limits imposed by physics. There are impressive estimates for latency for 5G, but reality has a tendency to be harsh on such predictions. Reliability will be an even bigger problem than latency. Remember your last trip to New York or San Francisco? Cell service in major cities is often poor because signals are reflected from buildings and attenuated (weakened) as they pass through. Those problems get worse as you go higher in frequency, as 5G does. Whether you’re interested in AVs or some other applications, making mobile connections more reliable is more important than making them faster. 5G intends to do so by trading off congestion against signal quality. That’s a plausible tradeoff, but it remains to be seen whether it works. Pete Warden, who is working on machine learning for very low power devices, says 5G is only marginally useful for the applications he cares about. When you’re trying to build a device that will run for months on a coin battery, you realize that the radio takes much more power than the CPU. You have to keep the radio off as much as possible, transmitting data in short, brief bursts. So what about industrial IoT (IIoT), and sensors that can be built into a sticker and slapped on to machinery? That might be a 5G application–but as Warden has said, the real win here is eliminating batteries and power cords, which in turn requires careful use of low-power networking. 5G isn’t ideal for that, and the first indications are that it will require more power than current technologies. Regardless of power consumption, I’m not convinced we’ll have lots of IoT devices shipping data back to their respective motherships. We’ve seen the reaction to news that Amazon’s Echo and Google Home send recordings of conversations back to the server. And we’re already seeing devices like smart thermostats and light bulbs being used for harassment. As privacy regulation takes hold and techniques like federated learning become more widespread, the need–and desire–for shipping our data far and wide will inevitably decrease. So where is 5G useful? Let’s get back to home networking. I’d gladly give up my 50 Mbps wired connection for gigabit wireless. Again, that’s the ultimate cable cutting, and it creates significant new possibilities. I might not want to watch 4K video on my phone (given current screen technology, to say nothing of our eyes’ angular resolution, high-resolution video on a phone is meaningless), but I might want to send video from my phone to my television using Chromecast. I’m satisfied with my current Wi-Fi deployment, but I wonder whether I’d even need Wi-Fi in a 5G world. Perhaps, for security and privacy reasons, it makes sense to separate a local network from the rest of the world. But that’s also a problem that 5G vendors could solve; virtual LANs (VLANs) are hardly a new concept. Gigabit connectivity to laptops, with the cell network providing a VLAN, could also replace office networks. In either case, some hard guarantees about privacy and security would be needed. Given service providers’ records on user tracking, that may be too much to ask. If we can get some enforceable guarantees about privacy and security on ISP-provided VLANs, I can imagine bigger changes. I’ve long thought it makes little sense to maintain disk drives (whether rust-based or solid-state) that periodically fail and need to be backed up. I do regular backups, but I know I’m the exception. What would the world look like if all of our storage was in the cloud, and access to that storage was so fast we didn’t care? What if all of your documents were in Google Docs, all of your music was in your favorite streaming service? That vision isn’t entirely new; Sun Microsystems had the idea back in the 1990s, and that’s essentially the vision behind Google’s Chromebooks. How would our usage patterns change with 5G? I have 30 or 40 GB of photos. I could upload them all to Google Photos or some other service, but at 50 Mbps down and 10 Mbps up, that’s not something you want to think about. At a gigabit, you don’t have to think twice. I’ve always been unimpressed by streaming services for music and video, at least partly because they’re least available when you most want them: when you’re flying or on a train, in at a technical conference with 3,000 attendees maxing out the hotel’s network. (Someone once told me “so download everything you’re likely to want to listen to before leaving.”\\xa0 Really.)\\xa0 But with gigabit microcells, this suddenly makes sense. Maybe not on flights, which are out of range of cell towers and where WoeFi will remain the order of the day, and maybe not when you’re driving through rural areas, but if I can get a gigabit network to my phone, why should I care about Amtrak’s slow Wi-Fi or network congestion in my hotel? If an office can get that kind of bandwidth to my laptop, with adequate guarantees for cloud security, why should we worry about office LANs? Whether that excites you or not, that strikes me as a significantly new pattern: we won’t care where our data is. We won’t need to worry about backups. We won’t need to worry (as much) about outages. We can bring our networks with us. We won’t even need to worry as much about security; Google, Amazon, and Microsoft all do better backups than I ever will, are much better at surviving network disruption, and know an awful lot more about how to protect my data. If Google can push their users to two-factor authentication (2FA) or the use of a security dongle, that’s a huge step toward safe computing. Those cloud providers will, of course, have to guarantee this data remains private–as private as it is when it lives on a personal disk drive or an office fileserver. That’s a problem that’s eminently solvable. The implications for business are even more important. Home users think in gigabytes; businesses are increasingly involved with tera- or petabytes. It’s a lot easier to move large datasets when you have ubiquitous gigabit networks. Whether that’s training data for AI applications or just lots of transaction records, businesses move data, and lots of it. With our current technology, the best way to move a huge amount of data is, all too often, to put disk drives on a truck. 5G brings us a lot closer to solving that problem–if we can get hard guarantees about security and privacy. Businesses are even less likely than users to appreciate some third party using their data for their own purposes. I’m sure that 5G will also lead to a new generation of smart devices that can use the bandwidth–devices we haven’t imagined yet. But I’m more interested in something I can imagine, decoupling myself from my data: having access to it any time, any where, without carrying it around, or stashing it on some kind of machine in the closet. That’s the real promise of 5G. — Mike Loukides We recently conducted a survey on serverless architecture adoption. We were pleasantly surprised at the high level of response: more than 1,500 respondents from a wide range of locations, companies, and industries participated. The high response rate tells us that serverless is garnering significant mindshare in the community. Key findings from the serverless survey include: Read “O’Reilly serverless survey 2019: Concerns, what works, and what to expect” for full results. Also be sure to check out our archive of Radar research and analysis. O’Reilly conferences combine expert insights from industry leaders with hands-on guidance about today’s most important technology topics. We hope you’ll join us at our upcoming events: O’Reilly Software Architecture Conference in New York, February 23-26, 2020 Strata Data Conference in San Jose, March 15-18, 2020 O’Reilly Artificial Intelligence Conference in San Jose, March 15-18, 2020',\n",
              " 'In this interview from O’Reilly Foo Camp 2019, Dean Wampler, head of evangelism at Anyscale.io, talks about moving AI and machine learning  into real-time production environments. Highlights from the interview include: Facilitating the transition from research to production in a robust way introduces a number of complications, Wampler says, including governance, GDPR, and traceability rules. Noting the importance of traceability, he offers an example: “If I deploy a model that’s making credit card authorizations, and I keep rejecting someone’s card, and they come on and say, ‘I’m a member of a minority group, and you keep turning down my charges. Are you prejudiced against me?’ or something like this, I need to know exactly what model was used and how it was trained. There are all kinds of logistical issues that have to be addressed in a real-world production environment.” (01:15) In some cases, AI and machine learning technologies are being used to improve existing processes, rather than solving new problems. Wampler used car loan approvals as an example: “It used to take a day or so to get an auto loan, and that worked. You could just come back to the dealer the next day and dream about your beautiful car that night but not actually have it. Companies like Capital One have gotten that [loan approval process] down to seconds. You can get on the app and get an approval for a loan immediately. So, it’s not something that had to be done in a real-time context, but it changed the world, changed their business being able to do that. There’s a lot of these sort of pragmatic examples.” (02:22) Wampler also discussed his personal interest in climate change and how individuals and businesses can use AI and machine learning tools to have a more significant influence than one might think. “What I’ve learned is there are a lot of little ways and big ways that add up when we’re working on stuff like this. One of the promises of tools like artificial intelligence is that it can automate human-level activity in a way that would not be feasible with actual humans doing it. More specifically, organizations like Google are already using sophisticated analytics to reduce the amount of energy they use and more efficiently utilize their machines. Individually, things like that are not going to solve climate change, but they add up. Every ton of carbon that you didn’t burn is one step in a solution toward the problem of climate change. For all of us, it really comes down to a whole spectrum of little things we can do that add up, from personal things like how we use energy, heat our homes, cook our food, and so forth, to thinking carefully about how we do our jobs and how we can be efficient in operationalizing these things, thinking about how we can help our customers achieve that, and then figuring out ways that we can have more direct influences.” (04:20)',\n",
              " 'I’m kicking things off with three quick thoughts for the start of the new year. These aren’t predictions. Rather, these are thoughts that can’t help but be right, and that take a bigger perspective on what’s happening.',\n",
              " 'In a classic 1983 paper, cognitive psychologist Lisanne Bainbridge drew attention to a curious irony: so-called “automated” systems were, in practice, usually attended by one or more human operators. The more advanced the system, she observed, “the more crucial…the contribution of the human operator.” Bainbridge believed that automation designers were in denial about this, however. As she saw it, designers approached the problem of automation as if the human factor were not, in fact, a factor. This resulted in systems that left their (inevitable) human operators “with an arbitrary collection of tasks” with respect to which “little thought may have been given to providing support.” This is precisely the kind of problem that robotic process automation (RPA) aims to address. To understand what RPA is, why it matters, and why it’s garnered so much interest, you have to understand the case of “Maddie.” Full disclosure: Maddie is a fictional person. She’s a composite of hundreds of thousands (perhaps even millions) of smart, industrious people who are tasked with the IT equivalent of rolling a large boulder up a hill. One of Maddie’s responsibilities—she has plenty of others—is to manipulate a mouse and click the appropriate buttons (“OK,” “Yes,” “Import”) when prompted to do so by a legacy Windows application. Once an hour, on the hour, Maddie sits in front of an old-school beige computer and clicks a mouse button several hundred times. She’s the vestigial human link in a process—insurance claims processing—that has a mostly automated workflow. Maddie has other responsibilities that challenge her ingenuity, but it’s the drudgery of pointing-and-clicking a mouse that defines her job for her. RPA is a beacon of hope for the Maddies of the world. It enlists software “robots” to automate the rote, repetitive, or tedious tasks that bridge virtual gaps, or facilitate virtual transfers or exchanges, in and between common business processes. Whether it’s copying and pasting text, scraping screens, pointing and clicking (or dragging and dropping) with a mouse, saving changes in one program and importing them into another, etc., software robots are able to interpret inputs, events, and gestures, and trigger responses and communicate with other systems—just as humans do. In fact, robots arguably do these things better than humans do them. Unlike humans, a software robot doesn’t get tired, bored, or distracted, especially when its performing a tedious or repetitive task. Unlike humans, a software robot performs tasks precisely the same way each and every time. Unlike humans, a software robot generates records for each and every task in a workflow. Most important, unlike humans, a robot does not have a capacity for creativity or ingenuity. Robots cannot reflect on and derive innovative solutions to hard problems. They can do only what they’re programmed to do. Core to RPA is the idea that the software robot is the perfect candidate for a range of tasks that neither tax human ingenuity nor engage human passion. Virtual automation has a long history. Prior to RPA, enterprises used several different techniques to automate workflows and back-office processes. Scripting, for example, is the ur-IT automation technology. Virtually anything can be scripted—including keyboard, mouse, and GUI actions. Speaking of mice, another automation technology—screen-scraping—dates from before the widespread use of graphical user interfaces (GUI), mice, and other even newer types of human interface devices. Even though mainframe technologies evolved—IBM’s System/360 mainframe gave way to System/370, which was superseded by System/380, and so on—the programs that ran on these systems often did not. In the late 1960s, following the shift from punch cards to magnetic tape, enterprises used optical character recognition (OCR) technologies to digitize their punch-card archives. In the 1980s, they used screen-scraping techniques to capture data from legacy mainframe and minicomputer programs and expose it in different contexts—such as on then-new PCs and Macintoshes. Two decades later, enterprises used screen-scraping to web-enable legacy mainframe, minicomputer, and client-server applications. Speaking of client-server, the succession of paradigm shifts from host-based to client-server to cloud-based architectures helped substantively automate many common workflows and processes. Yes, automation has usually eliminated a small proportion of human jobs; at the same time, however, it has created new opportunities for those well-versed in software, databases, architecture, and operations. But RPA is different. First, RPA automates tasks that a human being must perform in a GUI, as distinct to a text-only terminal. These are tasks that comprise gaps in the midst of critical workflows or processes; tasks that (as often as not) connect or span separate programs, systems, or networks. Second, RPA is smart; it uses machine learning (ML) techniques that learn on the basis of human action in the visual interface. Just by manipulating the mouse and clicking a sequence of “Yes,” “OK,” etc., prompts, a user like Maddie generates a training data set for a software robot to work with and learn from. The Achilles heel of scripts, mouse macros, and other established practices is that they all depend on human prognostication. Imagine a developer writes a script that triggers an action in response to one or more events: say, for example, some combination of function calls and GUI events. For the script to work reliably, however, the developer must anticipate not only expected events (first, the script moves the cursor to the designated X,Y coordinates; second it checks to make sure the designated window is activated; only then does it simulate a mouse click) but exceptions, too—edge cases, as when a dialog box captures control of the screen and obscures the “OK” prompt. RPA technology uses ML to discover, understand, and adapt to edge cases that a human user manipulating a mouse could easily deal with—but which would bring a dumb script grinding to a halt. Third, RPA doesn’t assume that the primary goal of automation is to replace human beings. RPA technologies automate the kinds of tasks that human beings don’t like doing. RPA targets tedious, repetitive, rote, time-consuming tasks. Proponents like to claim that RPA frees human actors (like Maddie) to focus on responsibilities and activities that engage their ingenuity and creativity. Fourth, RPA expects program, system, and even network heterogeneity. RPA evolved to eliminate gaps in workflows or processes that span disparate GUI-based systems. A history lesson might be helpful here. In the 1990s, packaged software suites emerged to displace fit-for-purpose GUI and text-based applications. An insurance company might once have depended on a mix of custom-built and commercial systems to support key processes such as enrollment, billing, claim filing, and claim adjustment; by the late-1990s, however, packaged applications were able to replicate many (if not most) of the features and functions these systems provided. But not all of them. More important, some subset of function-specific systems just couldn’t be replaced. The upshot was that even as enterprises restructured their business processes to accommodate packaged suites, they kept some of these processes (and their supporting IT systems) intact, too. The neat thing about RPA is that its software bots run alongside the GUI-based program(s) on the existing system. RPA does not modify the code of an existing system; it doesn’t even require that someone (a developer, engineer, etc.) understand this code. Software bots on one system can also interoperate with software bots on other systems; this permits RPA to knit together disparate systems into a single (managed) process flow. All of this helps account for keen and trending interest in RPA. In most cases, RPA is a less expensive alternative to conventional practices: to approximate its efficacy (and to account for most possible problems, hiccups, or exceptions) a developer would have to observe what a user like Maddie does over a protracted period of time. RPA also delivers return on investment via increased employee productivity, improvements in process efficiency, built-in auditability, and superior accuracy. (Humans grow bored, get distracted, and don’t notice when they make mistakes. When a software bot makes a “mistake,” it triggers an exception.) One critical factor for success with any RPA project is exception handling. In practice, the problem of anticipating and controlling for exceptions—i.e., anomalous events—constitutes the most important and challenging aspect of implementing an RPA project. Software robots must learn to identify and control for scenarios that human beings take for granted, such as slow, non-responsive, and frozen applications—or, worse, application errors or crashes. But controlling for application performance, availability, and dependency issues is extremely complicated. One approach is for software robots to poll for resource availability and collect detailed statistics about the host operating system’s state. The robot will use this data to train an ensemble of machine learning (ML) models to identify and handle exceptions. In most cases, however, human beings, not ML models, devise the rules that robots will use to determine what to do when a program is unresponsive or unavailable. How long should the robot wait for a response? Does the robot kill and re-spawn unresponsive programs or processes? These are just a few of the questions a designer must anticipate and control for. The edge cases can quickly multiply, especially in scenarios in which critical programs and operating system libraries are implicated. And that’s the thing: all conceivable exceptions must be accounted for (and built into) the RPA design. An event that might cause a human being to do a double take can bring an RPA process to a complete halt.\\n RPA automates actions, inputs, behaviors, etc., in the user interface. It is ideal for integration projects that require human manipulation of user interface elements and involve heterogeneous systems. RPA technologies exploit standard[1] APIs, so integrators won’t have to tinker with existing applications, workflows, processes, or system architecture. In this way, RPA can help reduce the amount of work (along with much of the risk) necessary to integrate heterogeneous applications and systems. This is one of the biggest factors driving the cost-effectiveness of RPA implementations. RPA is not a panacea. Some tasks, even if tedious or repetitive, will require manual human oversight and control. As always, careful analysis of in-process workflows enables you to determine the best overall candidates for robotic automation. In general, the following are good places to start with RPA: The above benefits can lower costs, improve efficiency and quality, and can economically scale, all adding up to significant return on investment for the right RPA projects. The following are probably not good candidates for RPA, however: Fundamentally, RPA is an integration and interoperability technology. There is not a one-size-fits-all blueprint for “doing” RPA. There can’t be: every company and every integration or interoperability scenario is different. However, thanks to RPA’s extensive use in the health care, insurance, oil and gas, and government verticals, would-be adopters can avail themselves of resources—e.g., case studies and ROI studies, along with industry-specific consulting and integration services—that will assist them as they plan their RPA implementations. As RPA sees widespread use across all verticals, adopters should expect to encounter complex, unpredictable, and challenging scenarios. Experience with and analysis of the RPA space leads to a formalized list of “Do’s and Don’ts” adopters can use to troubleshoot their RPA implementation projects. Teams play a very important role in the success of an RPA implementation. Two common RPA-specific job roles are RPA developer and RPA solutions architect; these specialists will augment and liaison with an internal IT team in implementing RPA. Over the last two to three years, RPA has emerged as a promising technology for workflow and back-office automation. Unlike older automation technologies—such as scripting and screen-scraping—RPA is designed to automate the kinds of tasks that are performed by human users who must interact with and manipulate elements in a graphical user interface. These include custom or proprietary applications that expose basic prompts (buttons, dialog boxes, etc.) or other prompts that expect a user to perform an action of some kind (enter text or number values, left- or right-click a mouse, press a sequence of keys on a keyboard) in response to a visual prompt. The return on investment for RPA projects can be substantial; this alone helps explain the keen interest in the topic. This interest shows no signs of abating. And with Microsoft’s announcement of a new RPA-oriented cloud offering, UI Flows, RPA seems poised for mainstream adoption. “Adoption” doesn’t necessarily equal success, however. Like any new technology, RPA’s potential rewards are offset by nontrivial risks. If you understand what RPA can and can’t do, you can make an informed decision about whether RPA is a viable option for you.',\n",
              " 'In this interview from O’Reilly Foo Camp 2019, Adventures in Coderland author Andrew Smith discusses the journey he’s taken while learning to code. Having started from a place of not knowing anything about coding, Smith says he went much deeper into it than he thought he would. Highlights from the interview include: Smith decided to write a book about what he’s learned, a sort of “I’m learning to code so you don’t have to” kind of tome. “I understood nothing about [coding] at all, about how it works, or what a line of code means,” he explained. “How does this thing that looks like algebra make something happen over there? To most of us, that’s a complete mystery. Given that we’re beginning to have a lot of conversations about what we think code should do, and, more crucially, what it shouldn’t do, I felt I needed to be informed about it to participate in those conversations. I thought, ‘Well, actually, so does everyone.’ People aren’t going to all learn to code, but I could do it on their behalf, so they don’t have to.” (00:15) In learning to code, Smith discovered he worked best with Python because he found it very approachable and clear. The biggest challenge, he noted, wasn’t so much the language itself, but having to learn a new way of thinking. “What you’re trying to do is think the way a computer thinks. That’s the really big hurdle, and that’s the difficult thing. … I’m anthropomorphizing, but trying to get into the mindset of a computer, something that we would do in our analog way very quickly by saying a sentence might take three, four, five, or six steps to communicate to the computer.” (06:25) Smith has also discovered a cross-over between code and his interest in music. “Recently when I was in the UK, I got involved with live coding—the program I use is called Sonic Pi, which basically turns your laptop into a musical instrument. … In the UK, ‘algorave’ has become a small underground movement in nightclubs. The performer live codes the music and there’s a screen behind them that shows the code.” (18:04)',\n",
              " 'In this interview from O’Reilly Foo Camp 2019, Hands-On Unsupervised Learning Using Python author Ankur Patel discusses the challenges and opportunities in making machine learning and AI accessible and financially viable for enterprise applications. Highlights from the interview include: The biggest hurdle businesses face when implementing machine learning or AI solutions is cleaning and preparing unstructured data that exists across silos. Patel says commoditized infrastructure from companies like Amazon and Google is one of the most significant advancements toward a solution in this area: “A lot of the work that data scientists would have to do in a custom way is now being done, basically, out of the box by API calls on one of these platforms.” (00:57) Open source is going to provide a “massive benefit” for businesses, Patel says. “In computer vision, for example, starting in 2012, those models were essentially open sourced, so a lot of businesses then got into the business of applying those computer vision models for specific use cases, like autonomous tracking vehicles. So, it’s going to be less about the models, per se—it’s going to be more about the use cases and applications of those models.” (01:57) Open source data and transfer learning are also enabling businesses to more easily move models into production and to achieve an ROI. Patel notes that when data sets are open sourced, “that means any firm that wants to work on the data set, instead of training their own models, is able to do that. Then you have pre-trained models you can do transfer learning with. If you take a language model, for example, that’s provided by Google’s BERT and apply it to a corpus of documents that is in your vertical—let’s say legal documents at a law firm—and you want to make it easier to process law documents as opposed to using paralegals. You can take the massively pre-trained language model, fine tune it on your legal corpus, and then deploy that as a solution. So, you’re able to see the ROI a lot faster—say in six to 12 months versus what previously would’ve taken three to five years because you would’ve had to train your own model from scratch. This idea of transfer learning, using large pre-trained models, fine tuning on your own corpus of text, that is where we’re going in the near future. I think that’s something most businesses should be very optimistic about.” (06:27)',\n",
              " 'We originally shared a selection of books relevant to the ongoing transformation of the economy, the world of work, the costs of capitalism, and how business gets done, but it was impossible to include all the titles we wanted to highlight. Thus, you get another compendium, this one perhaps a bit more eclectic than our first post (we hope) but just as elucidating and thought-provoking. University of Toronto economist Joshua Gans and Australian member of parliament Andrew Leigh, also an economist, question the market mechanisms that link technological progress to rising wealth inequality. They propose ideas to transform today’s winner-take-all economy into a more entrepreneurial and egalitarian society in their new book, Innovation + Equality: How to Create a Future That Is More Star Trek Than Terminator. In a recent opinion piece for the Hill, the authors assert that we’re “running out of excuses for high inequality.” You’ll find an excerpt from the book, followed by an interview with the authors, in the Economist. Yale Law School professor Daniel Markovits scrutinizes the great American faith that talent and hard work will result in success and upward mobility, and finds it entirely unjustified in The Meritocracy Trap: How America’s Foundational Myth Feeds Inequality, Dismantles the Middle Class, and Devours the Elite. His sobering revelations include the role of higher education in deepening the class divide, as the concentration of wealthy kids in prestigious schools continues to grow and the middle class continues to shrink, as is detailed in this New York Times review. In this interview from the Nation, Markovits explains how “the meritocracy is making us miserable.” Listen in as Markovits expands on his premise on The Ezra Klein Show, or read this adapted excerpt in the Atlantic. Columbia Law School professor Katharina Pistor explains the many ways our legal system perpetuates today’s extreme concentration of wealth in The Code of Capital: How the Law Creates Wealth and Inequality. The Boston Review extolls Pistor’s tome along with two other alarming books that call out structural injustices and call for a thorough revision of economic imperatives. Pistor occasionally writes for Project Syndicate; in her latest piece she declares, “If wealth is justified, so is a wealth tax.” Technology journalist Clive Thompson, who writes for the New York Times Magazine and Wired, lays out how those who create the algorithms create the world and the ways we live in it in his book Coders: The Making of a New Tribe and the Remaking of the World. Thompson conveys the joy and artistry of programming along with the inevitable unintended consequences of innovation in this interview from The Verge. The New York Times review of Coders describes the book as “a journey—if you dare—into the minds of Silicon Valley programmers.” In Make, Think, Imagine: Engineering the Future of Civilization, John Browne, an engineering apprentice who rose through the ranks to become CEO of BP from 1995 to 2007 and served as president of the UK’s Royal Academy of Engineering, asserts that technological progress can and must be harnessed to better civilization and spread prosperity…even today, as visions of a techno-dystopia can seem overwhelming. Browne discusses his faith in engineering, technology, and innovation in this Book Talk hosted by Columbia University’s School of International and Public Affairs. This Financial Times review emphasizes Browne’s faith in imagination, planning, and design to build a better society. While it’s not a new book, we didn’t cover Learning by Doing: The Real Connection Between Innovation, Wages, and Wealth when it was published in 2015, and it deserves a place in our Next Economy reading list. Tim O’Reilly liked it so much that he devoted a chapter to it in his own 2017 book, WTF? What’s the Future and Why It’s Up to Us. Entrepreneur and economist James Bessen, the executive director of the Technology & Policy Research Initiative at Boston University School of Law, extolls learning on the job as the path to prosperity and advocates for policy changes that provide incentives for employers to put their workers on that path. Bessen was a speaker on the series Talks at Google in 2017 and a guest on Russ Roberts’s EconTalk podcast in 2016. The Atlantic adapted an excerpt from his book: “Scarce Skills, Not Scarce Jobs.” In his 2018 book, Hedge: A Greater Safety Net for the Entrepreneurial Age, Nicolas Colin, cofounder and director of the European investment firm The Family and a prolific writer, argues that as we progress from the industrial age into an evermore digital world of work—one of faster-growing companies, rampant entrepreneurship, and a growing population of independent workers and gig economy participants—it’s increasingly important for society to rethink and reinforce its safety net. In this Medium post, Colin raises the prospect that China—not the US—will be the architect of “the Great Safety Net 2.0.” He makes the case that Silicon Valley should throw out its playbook and start caring about the world in this essay from Forbes. Colin made an appearance on Talks at Google this spring to discuss Hedge and share his ideas about technology, policy, and the need to upgrade our institutions. Cryptocurrencies have been subject to scandal and subterfuge, but the decentralized, distributed ledger technology underpinning the rise of Bitcoin and its brethren (a.k.a. the blockchain) has evolved to be used in many industries for authentication of information. Kevin Werbach, professor of legal studies and business ethics at the Wharton School at the University of Pennsylvania, recounts this evolution and describes the potential of the blockchain as a type of legal technology that can create tremendous business and social value in his book The Blockchain and the New Architecture of Trust. Werbach weighed in on Facebook’s now back-burnered cryptocurrency, Libra, in the New York Times, positing it as “the social network’s last, best hope to regain public trust.” The New York Times covered Werbach’s book, along with another blockchain book, in this review. Here’s Werbach’s Talks at Google appearance to discuss his book. Georgetown University computer science professor Cal Newport had an instant hit on his hands with Digital Minimalism: Choosing a Focused Life in a Noisy World; he became known as the Marie Kondo of mobile phones for his sensible advice for staying centered in a world filled with addictive devices by making intentional choices and using tech to support your goals rather than enabling it to use you. Word of this book spread far beyond the usual Next Economy media suspects. Newport took to the pages of Outside with tips for readers to maximize their free time: “To Upgrade Your Leisure, Downgrade Your Phone.” GQ interviewed Newport about Digital Minimalism and “why we’ll look back at our smartphones like cigarettes.” New Yorker writer Jia Tolentino road-tested Newport’s advice to find out firsthand “what it takes to put your phone away.” Our tireless editors don’t just sit around reading great books; they’re continually on the lookout for savvy experts who can translate their skills and knowledge into books, videos, online courses, and other learning experiences to give you the most effective tools to master what you need to know to thrive in the Next Economy. Here are a few new tools to sharpen your competitive edge.',\n",
              " 'We see the AI space poised for an acceleration in adoption, driven by more sophisticated AI models being put in production, specialized hardware that increases AI’s capacity to provide quicker results based on larger datasets, simplified tools that democratize access to the entire AI stack, small tools that enables AI on nearly any device, and cloud access to AI tools that allow access to AI resources from anywhere. Integrating data from many sources, complex business and logic challenges, and competitive incentives to make data more useful all combine to elevate AI and automation technologies from optional to required. And AI processes have unique capabilities that can address an increasingly diverse array of automation tasks, tasks that defy what traditional procedural logic and programming can handle—for example: image recognition, summarization, labeling, complex monitoring, and response. In fact, in our 2019 surveys, more than half of the respondents said AI (deep learning, specifically) will be part of their future projects and products—and a majority of companies are starting to adopt machine learning. Access to the amount of data necessary for AI, proven use cases for both consumer and enterprise AI, and more-accessible tools for building applications have grown dramatically, spurring new AI projects and pilots. To stay competitive, data scientists need to at least dabble in machine and deep learning. At the same time, current AI systems rely on data-hungry models, so AI experts will require high-quality data and a secure and efficient data pipeline. As these disciplines merge, data professionals will need a basic understanding of AI, and AI experts will need a foundation in solid data practices—and, likely, a more formal commitment to data governance. That’s why we decided to merge the 2020 O’Reilly AI and Strata Data Conferences in San Jose, London, and New York. We’re in a highly empirical era for machine learning. Tools for machine learning development need to account for the growing importance of data, experimentation, model search, model deployment, and monitoring. At the same time, managing the various stages of AI development is getting easier with the growing ecosystem of open source frameworks and libraries, cloud platforms, proprietary software tools, and SaaS. While deep learning continues to drive a lot of interesting research, most end-to-end solutions are hybrid systems. In 2020, we‘ll hear more about the essential role of other components and methods—including Bayesian and other model-based methods, tree search, evolution, knowledge graphs, simulation platforms, and others. We also expect to see new use cases for reinforcement learning emerge. And we just might begin to see exciting developments in machine learning methods that aren’t based on neural networks. Developments in computer vision and speech/voice (“eyes and ears”) technology help drive the creation of new products and services that can make personalized, custom-sized clothing, drive autonomous harvesting robots, or provide the logic for proficient chatbots. Work on robotics (“arms and legs”) and autonomous vehicles is compelling and closer to market. There’s also a new wave of startups targeting “traditional data” with new AI and automation technologies. This includes text (new natural language processing (NLP) and natural language understanding (NLU) solutions, chatbots, etc.), time series and temporal data, transactional data, and logs. And traditional enterprise software vendors and startups are rushing to build AI applications that target specific industries or domains. This is in line with findings in a recent McKinsey survey: enterprises are using AI in areas where they’ve already invested in basic analytics. Taking a cue from the software quality assurance world, those working on AI models need to assume their data has built-in or systemic bias and other issues related to fairness—like the assumption that bugs exist in software, and that formal processes are needed to detect, correct, and address those issues. Detecting bias and ensuring fairness doesn’t come easy and is most effective when subject to review and validation from a diverse set of perspectives. That means building in intentional diversity to the processes used to detect unfairness and bias—cognitive diversity, socioeconomic diversity, cultural diversity, physical diversity—to help improve the process and mitigate the risk of missing something critical. Deepfakes have tells that automated detection systems can look for: unnatural blinking patterns, inconsistent lighting, facial distortion, inconsistencies between mouth movements and speech, and the lack of small but distinct individual facial movements (how Donald Trump purses his lips before answering a question, for example). But deepfakes are getting better. As 2020 is a US election year, automated detection methods will have to be developed as fast as new forms of machine deception are launched. But automated detection may not be enough. Detection models themselves can be used to stay ahead of the detectors. Within a couple months of the release of an algorithm that spots unnatural blinking patterns for example, the next generation of deepfake generators had incorporated blinking into their systems. Programs that can automatically watermark and identify images when taken or altered or using blockchain technology to verify content from trusted sources could be a partial fix, but as deepfakes improve, trust in digital content diminishes. Regulation may be enacted, but the path to effective regulation that doesn’t interfere with innovation is far from clear. As AI tools become easier to use, AI use cases proliferate and AI projects are deployed, and cross-functional teams are being pulled into AI projects. Data literacy will be required from employees outside traditional data teams—in fact, Gartner expects that 80% of organizations will start to roll out internal data literacy initiatives to upskill their workforce by 2020. But training is an ongoing endeavor, and to succeed in implementing AI and ML, companies will need to take a more holistic approach toward retraining their entire workforces. This may be the most difficult, but most rewarding, process for many organizations to undertake. The opportunity for teams to plug into a broader community on a regular basis to see a wide cross-section of successful AI implementations and solutions is also critical. Retraining also means rethinking diversity. Reinforcing and expanding on how important diversity is to detecting fairness and bias issues, diversity becomes even more critical for organizations looking to successfully implement truly useful AI models and related technologies. As we expect most AI projects to augment human tasks, incorporating the human element in a broad, inclusive manner becomes a key factor for widespread acceptance and success.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7JuwNTW4JIn",
        "colab_type": "text"
      },
      "source": [
        "### Sentence tokenize each document in the list of documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zbqurmN4JIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_list=[]\n",
        "for doc in doc_list:\n",
        "    token = sent_tokenize(doc)\n",
        "    sent_list.append(token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL4tEWBRAvwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1dc920f-95ad-4b9e-922a-5f293975619c"
      },
      "source": [
        "len(sent_list[0][0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqT7V6zZ4JIu",
        "colab_type": "text"
      },
      "source": [
        "### Word tokenize each sentence within each document.\n",
        "\n",
        "You should end up with a nested list structure where the outer list contains all the sentences in each document and the inner list contains the tokenized sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17JMEoHe4JIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenez_list=[]\n",
        "for doc  in sent_list:\n",
        "    temp_list=[]\n",
        "    for sent in doc:\n",
        "        token = word_tokenize(sent)\n",
        "        temp_list.append(token)\n",
        "    tokenez_list.append(temp_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpjbg5Xl5kEN",
        "colab_type": "code",
        "outputId": "93d3b7fa-6a1f-43be-f4c4-1775e84889eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tokenez_list[0][0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ygIcSTP1dlU",
        "colab_type": "code",
        "outputId": "4782ff50-50f8-4c17-f2c9-1f0942997c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk1r_y4C4JIz",
        "colab_type": "text"
      },
      "source": [
        "### Tag each token with its part of speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meV3lfJR00tD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " tagged_list=[]\n",
        " for docid in corpus_rss.fileids():   \n",
        "    doc = corpus_rss.raw(docid)\n",
        "    sents = sent_tokenize(doc)\n",
        "    tokenized = [word_tokenize(sent) for sent in sents]\n",
        "    tagged = [pos_tag(tokens) for tokens in tokenized]\n",
        "    tagged_list.append(tagged)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9tKMKJV6Ecy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagged_list=[]\n",
        "for doc  in tokenez_list:\n",
        "    temp_list=[]\n",
        "    for sent in doc:\n",
        "        token = pos_tag(sent)\n",
        "        temp_list.append(token)\n",
        "    tagged_list.append(temp_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRaPDFw-52BS",
        "colab_type": "code",
        "outputId": "5095a7f8-1a46-4656-f344-8da9ef3d5afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "tagged_list[0][0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Look', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('industry', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('become', 'VB'),\n",
              " ('more', 'RBR'),\n",
              " ('stratified', 'JJ'),\n",
              " ('and', 'CC'),\n",
              " ('specialized', 'JJ'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmIltOA44JI5",
        "colab_type": "text"
      },
      "source": [
        "### Word tokenize the raw text of each document and remove stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg8ciX5G4aa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVUcQFkJ4JI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_list_without_stop=[]\n",
        "for docid in corpus_rss.fileids():\n",
        "    doc = corpus_rss.raw(docid)\n",
        "    doc = nlp(doc)\n",
        "    doc_without_stop = [token for token in doc if not token.is_stop]\n",
        "    #print(doc_without_stop)\n",
        "    doc_list_without_stop.append(doc_without_stop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbnilx_W8Cer",
        "colab_type": "code",
        "outputId": "e2502fdc-85e7-44ce-c539-f91422718696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GX001336ibc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "without_stop_list=[]\n",
        "for doc  in doc_list:\n",
        "    token = word_tokenize(doc)\n",
        "    temp_list=[]\n",
        "    for word in token:\n",
        "        if (word.lower() not in stopwords.words('english')):\n",
        "            temp_list.append(word.lower())\n",
        "    without_stop_list.append(temp_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Qp6vSV8Jar",
        "colab_type": "code",
        "outputId": "927fc31a-17eb-4a95-d0fa-caa55b87e73e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "without_stop_list[0][0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'look'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAZggM574JI_",
        "colab_type": "text"
      },
      "source": [
        "### For every document, stem all the words in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z_VSny98qVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgLS7PPD4JJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmed_list =[]\n",
        "for doc in without_stop_list:\n",
        "    stemmed = [stemmer.stem(word) for word in doc]\n",
        "    stemmed_list.append(stemmed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2N4mHrZ4JJQ",
        "colab_type": "text"
      },
      "source": [
        "### Iterate through each document, computing and printing the following document statistics for each.\n",
        "\n",
        "- Number of sentences\n",
        "- Average words per sentence\n",
        "- Vocabulary\n",
        "- Lexical Diversity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WdodCrX9NcB",
        "colab_type": "code",
        "outputId": "99a79d51-abb4-4a8f-d49e-65b3fa76f8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "for doc in tokenez_list:\n",
        "    sentences = len(doc)\n",
        "    print('# of sentences', sentences)\n",
        "    avg_words = sum([len(sent) for sent in doc])/sentences\n",
        "    print('Avg word per sent', avg_words)\n",
        "    vocab = len(set([w.lower() for w in word_tokenize(str(doc))]))\n",
        "    print('Unique words', vocab)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of sentences 39\n",
            "Avg word per sent 23.384615384615383\n",
            "Unique words 392\n",
            "# of sentences 50\n",
            "Avg word per sent 39.36\n",
            "Unique words 806\n",
            "# of sentences 60\n",
            "Avg word per sent 36.583333333333336\n",
            "Unique words 807\n",
            "# of sentences 33\n",
            "Avg word per sent 35.54545454545455\n",
            "Unique words 440\n",
            "# of sentences 60\n",
            "Avg word per sent 25.75\n",
            "Unique words 611\n",
            "# of sentences 89\n",
            "Avg word per sent 25.213483146067414\n",
            "Unique words 701\n",
            "# of sentences 63\n",
            "Avg word per sent 24.238095238095237\n",
            "Unique words 517\n",
            "# of sentences 30\n",
            "Avg word per sent 32.733333333333334\n",
            "Unique words 394\n",
            "# of sentences 96\n",
            "Avg word per sent 22.958333333333332\n",
            "Unique words 714\n",
            "# of sentences 17\n",
            "Avg word per sent 34.05882352941177\n",
            "Unique words 271\n",
            "# of sentences 3\n",
            "Avg word per sent 16.666666666666668\n",
            "Unique words 42\n",
            "# of sentences 102\n",
            "Avg word per sent 24.323529411764707\n",
            "Unique words 815\n",
            "# of sentences 15\n",
            "Avg word per sent 30.533333333333335\n",
            "Unique words 217\n",
            "# of sentences 12\n",
            "Avg word per sent 39.416666666666664\n",
            "Unique words 222\n",
            "# of sentences 32\n",
            "Avg word per sent 43.125\n",
            "Unique words 578\n",
            "# of sentences 42\n",
            "Avg word per sent 28.833333333333332\n",
            "Unique words 525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S-KGZ6B-Q0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_tokenize(str(tokenez_list[0][0]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}