{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Day 74, Lecture 1: Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4I54bZzMImI",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning: Text Classification Assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBhHdzKtMImK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tVDJJaMImN",
        "colab_type": "text"
      },
      "source": [
        "### Use the CategorizedPlaintextCorpusReader to import the movie reviews corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "261oLyYyEORa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import movie_reviews"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLiMeG5cIk1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a81ddcc1-4d5e-4c31-f83f-fee2f7f91f26"
      },
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REtoZb_iMImO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')\n",
        "DOC_PATTERN = r'.*\\.txt'\n",
        "CAT_PATTERN = r'([\\w_\\s]+/.*)'\n",
        "\n",
        "corpus = PlaintextCorpusReader(, DOC_PATTERN)\n",
        "corpus = CategorizedPlaintextCorpusReader('path', DOC_PATTERN, cat_pattern=CAT_PATTERN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQMuBvquMImP",
        "colab_type": "text"
      },
      "source": [
        "### Create two separate lists - one containing the text from each document and another containing the category of each article in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGT8S_OfSWV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
        "\n",
        "categories = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauhoroBI85E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_reviews.fileids()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV6Ty7nuJND8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGRABGW8MImR",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess the corpus, ensuring to include the following steps.\n",
        "\n",
        "- Word tokenize the documents.\n",
        "- Lemmatize, stem, and lowercase all tokens.\n",
        "- Remove punctuation and stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeQiey8uJnhS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "25fe8f63-bceb-4479-c97b-b26335475dd4"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W7ak-puCVX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(docs):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    preprocessed = []\n",
        "\n",
        "    for doc in docs:\n",
        "        tokenized = word_tokenize(doc)\n",
        "        cleaned = [stemmer.stem(lemmatizer.lemmatize(token.lower()))\n",
        "                    for token in tokenized\n",
        "                    if token.lower() not in stopwords.words('english')\n",
        "                    if token.isalpha()]\n",
        "        untokenized = \" \".join(cleaned)\n",
        "        preprocessed.append(untokenized)\n",
        "    return preprocessed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8nKN842DbH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_movie = preprocess(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZE-q4ziMImT",
        "colab_type": "text"
      },
      "source": [
        "### Split the data into training and testing sets with the size of the test set being 30% of the records."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKFyBgjBMImU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = tts (text_movie, categories, test_size=0.3, random_state=23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reegQu_5MImV",
        "colab_type": "text"
      },
      "source": [
        "### Construct a pipeline that TF-IDF vectorizes the text and trains a Random Forest classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3JJ3hjNMImW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "b1bc8014-0a8a-492c-cebf-0ed433ed2d15"
      },
      "source": [
        "model = Pipeline([('vect', CountVectorizer()),\n",
        "                  ('tfidf', TfidfTransformer()),\n",
        "                  ('clf', RandomForestClassifier()),])\n",
        "model.fit(X_train,y_train)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=Non...\n",
              "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                        class_weight=None, criterion='gini',\n",
              "                                        max_depth=None, max_features='auto',\n",
              "                                        max_leaf_nodes=None, max_samples=None,\n",
              "                                        min_impurity_decrease=0.0,\n",
              "                                        min_impurity_split=None,\n",
              "                                        min_samples_leaf=1, min_samples_split=2,\n",
              "                                        min_weight_fraction_leaf=0.0,\n",
              "                                        n_estimators=100, n_jobs=None,\n",
              "                                        oob_score=False, random_state=None,\n",
              "                                        verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFt6djjpMImX",
        "colab_type": "text"
      },
      "source": [
        "### Generate predictions on the test set and print a classification report to evaluate how well the model performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWT99tvHMImY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "bb3fd52b-bc09-4702-8461-df4c82684556"
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.76      0.85      0.80       305\n",
            "         pos       0.82      0.72      0.77       295\n",
            "\n",
            "    accuracy                           0.79       600\n",
            "   macro avg       0.79      0.78      0.78       600\n",
            "weighted avg       0.79      0.79      0.78       600\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2BwKeBNMImZ",
        "colab_type": "text"
      },
      "source": [
        "### Perform 10-fold cross validation and obtain the averge F1 score across all the folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9cTvc6MIma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87442044-95e7-4189-ccc7-423e9a053330"
      },
      "source": [
        "scores = cross_val_score(model,text_movie,categories, cv=10, scoring = 'f1_macro')\n",
        "scores.mean(), scores.std()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7976178854770497, 0.013770689922035404)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H9ERShiMImb",
        "colab_type": "text"
      },
      "source": [
        "### Ingest, preprocess, and predict the topic of the article at the following URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2mfYGSMImc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://www.nytimes.com/2019/11/25/business/uber-london.html'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cQwDMu1PRtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://www.10news.com/lifestyle/exploring-san-diego/review-rise-of-the-resistance-fulfills-missing-star-wars-feel-at-galaxys-edge'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7piaPcKNMImd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22358e99-4fd4-492f-a654-008a428b39f9"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_url_text(url):\n",
        "    response = requests.get(url)\n",
        "    content = response.text\n",
        "\n",
        "    TAGS = ['h1','h2','h3','h4','h5','h6','h7','p','li']\n",
        "    soup = BeautifulSoup(content, 'lxml')\n",
        "    text_list = [tag.get_text() for tag in soup.find_all(TAGS)]\n",
        "    text = ' '.join(text_list)\n",
        "    return text\n",
        "\n",
        "text = get_url_text(url)\n",
        "cleaned = preprocess([text])\n",
        "model.predict(cleaned)[0]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}